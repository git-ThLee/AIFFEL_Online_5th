{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9f371af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q nltk rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a26797",
   "metadata": {},
   "source": [
    "# 루브릭\n",
    "\n",
    "|평가문항|\t상세기준|\n",
    "|:---|:---|\n",
    "|1. 기존 데이터셋을 추가 정제하고, generation 성능을 끌어올리기 위한 기법들을 실험해 모델 perfomance를 향상시켜보았는가?|\t기존 데이터셋의 문제점을 분석하고 전처리 전략을 수립해 추가 정제를 진행했다. Beam search, Top-k(p) sampling 등 최선의 디코딩 전략을 수립해 향상된 모델 추론 결과를 제시했다. BLEU, ROUGE 등 생성된 텍스트를 평가하기 위한 메트릭을 적용한 정량적인 평가 결과와 주관적인 평가를 비교분석하였다.|\n",
    "|2. 새로운 데이터를 수집해 전처리를 수행하여 모델을 재학습시켜보았는가?\t|모두의 말뭉치, AI hub 등에 공개된 데이터를 사용해 추가 데이터셋을 구축하기 위한 기준과 근거를 수립했다. ChatGPT API나 다양한 한국어 benchmark 데이터셋을 활용해 Human Feedback 을 대체할 수 있는 아이디어를 구현했다. 위를 바탕으로 SFT, RM, PPO 세 단계에 필요한 각 데이터셋을 적절히 구축하여, 모델 추론 결과와 수립한 가설을 비교해보았다.|\n",
    "|3. 학습 전략 또는 foundation model을 변경해 모델을 재학습시켜보았는가?\t|더 적절한 Instruction Tuning 기법을 적용해 SFT를 해보거나, Reward Model의 ranking algorithm을 개선해보았다. KoGPT-2가 아닌 다른 모델을 initial model로 사용하여 모델 학습을 성공시켰다. 허깅페이스의 accelerate, bitsandbytes 라이브러리 등을 사용하여 더 큰 스케일의 모델로 ChatGPT를 re-building해 모델 성능을 향상시켰다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce0df1",
   "metadata": {},
   "source": [
    "# 개선 방향 제시\n",
    "\n",
    "1. 우리가 지난시간 살펴본 KoChatGPT 모델에 사용한 데이터셋은 아직 완벽히 정제되지 않았습니다.\n",
    "\n",
    "2. Hunman Feedback이 반영된 데이터셋을 대체하기 위해\n",
    "SFT와 RM 모델에 사용할 다양한 benchmark 데이터셋도 검토해볼 수 있습니다.\n",
    "\n",
    "3. 언어모델의 생성능력을 좌우하는 최선의 디코딩을 위한 하이퍼파라미터 서치가 필요합니다.\n",
    "\n",
    "4. 생성된 답변에 대한 주관적인 평가를 보완할 수 있는 정량적인 메트릭은 도입하지 않았었습니다.\n",
    "\n",
    "5. LLM Trend Note1에서 살펴본 다양한 Instruction Tuning 및 Prompting 기법들도 적용해볼만 합니다.\n",
    "\n",
    "6. 무엇보다 foundation model로 사용한 KoGPT-2는 Emergent abilities를 기대하기엔 다소 작은 사이즈의 모델입니다.\n",
    "더 큰 파라미터 스케일을 가진 모델을 사용해보거나,\n",
    "\n",
    "7. 더 효율적인 연산을 수행할 수 있는 LoRA의 적용 또는\n",
    "새로운 Instruction Tuning 및 reward ranking 알고리즘을 도입해볼 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9d2fd",
   "metadata": {},
   "source": [
    "# 00. 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "96fa15ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87706f5",
   "metadata": {},
   "source": [
    "# 00. 모델 & 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9f9da9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "model_name = 'skt/kogpt2-base-v2' # skt/ko-gpt-trinity-1.2B-v0.5\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    bos_token='</s>', \n",
    "    eos_token='</s>', \n",
    "    unk_token='</s>', \n",
    "    pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194f95f1",
   "metadata": {},
   "source": [
    "# 00. 데이터 & 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2b28bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        data_path_1_SFT = 'data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6ec783a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "49248dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac27775",
   "metadata": {},
   "source": [
    "# 00. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "03929827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"aiffel/KoChatGPT/test\",    # 학습 결과를 저장할 디렉토리 경로 지정 \n",
    "    overwrite_output_dir = True,     # 기존 디렉토리가 있는 경우 덮어쓸지 여부 설정 (True: 덮어쓰기, False: 덮어쓰지 않음) \n",
    "    num_train_epochs = 10,    # 전체 학습 에포크 수 설정  \n",
    "    per_device_train_batch_size = 8,    # 각 디바이스당 학습 배치 크기 설정 (GPU 또는 TPU 기준)\n",
    "    per_device_eval_batch_size = 8,    # 각 디바이스당 평가 배치 크기 설정 (GPU 또는 TPU 기준)    \n",
    "    warmup_steps = 5,    # 학습 시작 시 learning rate를 증가시키는 스텝 수 설정 (웜업 스텝)\n",
    "    prediction_loss_only = True,    # 예측 손실만 계산할 것인지 설정 (True: 예측 손실만 계산, False: 전체 손실 계산)\n",
    "    fp16 = True,    # Mixed precision 학습 사용 여부 설정 (True: 사용, False: 사용하지 않음)\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e184cb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 58:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.876400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.840700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.305400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.866500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.535300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.581100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.624800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.239500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.911500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.722300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.748500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.618400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.631300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.555500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.560600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f489f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('aiffel/KoChatGPT/output_1_SFT_10ep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa0ee1",
   "metadata": {},
   "source": [
    "# 00. 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "6243b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='aiffel/KoChatGPT/output_1_SFT_10ep', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=1,                   # 하나의 빔만 사용\n",
    "    repetition_penalty=2.0,         # 답변 중복을 피하기 위한 패널티 설정\n",
    "    no_repeat_ngram_size=1,         # 중복되는 n-gram 금지\n",
    "    eos_token_id=375,               # 문장 종료 토큰 설정 (모델에 맞게 수정)\n",
    "    max_new_tokens=64,              # 생성할 최대 토큰 수 설정\n",
    "    do_sample=False,                # 샘플링 \n",
    "    top_k=150,                      # 상위 k개 후보 중에서 선택\n",
    "    top_p=0.9,                      # 누클레어스 샘플링 확률 설정\n",
    "    early_stopping=False,            # 생성 중단 설정\n",
    "    temperature=1.0,               # 온도 설정 (더 낮게 설정하면 더 확실한 답변)\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "list_result = generator(list_prompt, **generation_args)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "24b55e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_label = [\n",
    "    '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.',\n",
    "    '1953년입니다.',\n",
    "    '시카고 오헤어 국제공항은 미국 일리노이 주 시카고에 위치해 있습니다.',\n",
    "    '미세먼지 농도는 어제와 비교해서 개선되었지만 아직도 나쁜 수준이며, 마스크 착용과 실외 활동 자제를 권장합니다. 정확한 미세먼지 농도를 확인하려면 해당 지역의 미세먼지 측정소에서 확인해보시기 바랍니다.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "e6e84d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q. 불고기용 고기 한우에요?\n",
      "\n",
      "G. 저는 인공지능 언어모델로써 답변을 드리고 있어서, 질문이 너무 모호합니다. \" 불고기 용 고기도 하나요?\"라는 질문이라면, 그것은 저의 지식 범위를 벗어나며, 일반적으로 사용되는 종류의 고기를 의미할 수 있습니다. 예를 들어, 소갈비 같은 돼지고기를 사용하는 가정에서는 보통 쇠고기의 양, 종류, 저장\n",
      "\n",
      "A. 저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\n",
      "\n",
      "1-Gram BLEU : 0.05128205128205127\n",
      "2-Gram BLEU : 2.2250738585072626e-308\n",
      "3-Gram BLEU : 2.2250738585072626e-308\n",
      "4-Gram BLEU : 2.2250738585072626e-308\n",
      "--------------------------------------------------\n",
      "Q. 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "G. 1953년입니다. 드나브 포필드(Drinard P. Pope)가 41 대 대통령 선거에서 출마해 39%의 득표율을 얻었고, 그 후보는 캠벨(Campbell) 후보가 되었습니다. 따라서 리처드닉슨은 47대의 대통령으로 재임한 것으로 알려져 있습니다. (출처: 클린턴 재단,\n",
      "\n",
      "A. 1953년입니다.\n",
      "\n",
      "1-Gram BLEU : 0\n",
      "2-Gram BLEU : 0\n",
      "3-Gram BLEU : 0\n",
      "4-Gram BLEU : 0\n",
      "--------------------------------------------------\n",
      "Q. 시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "G. 미국 일리노이 주 시카고에 위치해 있습니다.:\\n- 시카고에서 가장 중요한 항공사 중 하나입니다. (British Martinz)\", 'token’ : 81} young firm or Newsweet parkingtoni in English. Howev\n",
      "\n",
      "A. 시카고 오헤어 국제공항은 미국 일리노이 주 시카고에 위치해 있습니다.\n",
      "\n",
      "1-Gram BLEU : 0.04000000000000001\n",
      "2-Gram BLEU : 2.2250738585072626e-308\n",
      "3-Gram BLEU : 2.2250738585072626e-308\n",
      "4-Gram BLEU : 2.2250738585072626e-308\n",
      "--------------------------------------------------\n",
      "Q. 오늘 미세먼지 어때?\n",
      "\n",
      "G. 미세 먼지는 오늘 하루를 시작하는 데 매우 중요한 역할을 합니다. 특히 출근길이나 저녁 무리한 시간대의 교통 상황에 따라 대기 질이 악화될 수 있으므로, 마스크 착용과 실외 활동 자제를 권장합니다. 또한, 외출 후에는 공기 질을 개선하는 것도 중요하며, 건강한 식습관을 유지하는 것이 좋습니다. (202\n",
      "\n",
      "A. 미세먼지 농도는 어제와 비교해서 개선되었지만 아직도 나쁜 수준이며, 마스크 착용과 실외 활동 자제를 권장합니다. 정확한 미세먼지 농도를 확인하려면 해당 지역의 미세먼지 측정소에서 확인해보시기 바랍니다.\n",
      "\n",
      "1-Gram BLEU : 0.023255813953488372\n",
      "2-Gram BLEU : 2.2250738585072626e-308\n",
      "3-Gram BLEU : 2.2250738585072626e-308\n",
      "4-Gram BLEU : 2.2250738585072626e-308\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, result, label in zip(list_prompt, list_result, list_data_label):\n",
    "    #print(result)\n",
    "    q = prompt[prompt.index('\\n')+1:prompt.rindex('\\n')]\n",
    "    print('Q.', q )\n",
    "    g = result[0]['generated_text'][result[0]['generated_text'].rindex('응답')+5:]\n",
    "    \n",
    "    print('G.', g )\n",
    "    print()\n",
    "    a = label # 정답\n",
    "    print('A.', label)\n",
    "    print()\n",
    "    print(\"1-Gram BLEU :\", sentence_bleu(a.split(), g.split(), weights=(1, 0, 0, 0 )))  \n",
    "    print(\"2-Gram BLEU :\", sentence_bleu(a.split(), g.split(), weights=(0, 1, 0 ,0 )))  \n",
    "    print(\"3-Gram BLEU :\", sentence_bleu(a.split(), g.split(), weights=(0, 0, 1 ,0 )))  \n",
    "    print(\"4-Gram BLEU :\", sentence_bleu(a.split(), g.split(), weights=(0, 0, 0, 1 ))) \n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "f548d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "074f91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6f3a3",
   "metadata": {},
   "source": [
    "# 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb9e41",
   "metadata": {},
   "source": [
    "- Epoch : 1 or 3 \n",
    "    - Q : 불고기용 고기 한우에요?\n",
    "        - A  요약 : \n",
    "    - Q : 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
    "        - A 요약 : 리처드 닉슨은 부통력직을 41대에 수행했다고만 함. 즉, 애초에 틀린 정보를 제공함. 또한, 본 질문과는 상관없는 리처드 닉슨에 대한 정보를 알려줌.\n",
    "    - Q : 시카고 오헤어 국제공항은 어디에 있어?\n",
    "        - A 요약 : 라벨과 동일한 위치를 알려줌. 단, 뒤에 불필요한 정보가 아닌 이상한 중국어, 일본어 등이 나옴\n",
    "    - Q : 오늘 미세먼지 어때?\n",
    "        - A 요약 : 라벨과 거의 동일함, 단 디테일한 정보는 다름. 예를 들어, 라벨은 어제와 비교해서 미세먼지가 나쁜 수준이라고 알려주지만, 생성된 텍스트는 날씨 정보를 알 수 없다고 함. \n",
    "        \n",
    "---\n",
    "\n",
    "- Epoch : 10 \n",
    "    - Q : 불고기용 고기 한우에요?\n",
    "        - A  요약 : -\n",
    "    - Q : 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
    "        - A 요약 : \"1953년입니다. 딥러닝 모델이 정확한 답변을 제공할 수 없습니다\" 와 같은 답변을 함. 일단 년도는 정확하게 맞추지만, 그 뒤에 리처드 닉슨과 관련 없는 정보가 나옴. 주로 딥러닝 키워드가 핵심임.\n",
    "    - Q : 시카고 오헤어 국제공항은 어디에 있어?\n",
    "        - A 요약 : 공항의 위치는 정확하게 맞추지만, 불필요한 정보를 많이 출력함.\n",
    "    - Q : 오늘 미세먼지 어때?\n",
    "        - A 요약 : -\n",
    "        \n",
    "---\n",
    "\n",
    "## 최종 결론 :\n",
    "\n",
    "> 학습된 모델에서 옵션을 튜닝하면서 최적의 대답을 찾는 것도 중요하지만, 학습을 더 많이 해야함을 느낌. \n",
    "\n",
    "> 옵션을 아무리 튜닝해도 한계는 존재하며(정답 자체를 모르는 경우), BLEU를 통한 성능 파악도 어려움\n",
    "\n",
    "> 에폭을 10으로 학습하는 동안 Training Loss가 점차 계속 감소하는 것을 보아 더 많이 학습해도 괜찮다고 판단함. 단, Training Loss을 무조건 적으로 믿으면 안됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3f371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
