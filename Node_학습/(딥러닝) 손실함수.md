# 손실함수 (Loss Functions)

비선형 활성화 함수를 가진 여러 개의 은닉층을 거친 다음 신호 정보들은 출력층으로 전달됩니다. 이때 우리가 원하는 `정답`과 전달된 `신호 정보들` 사이의 `차이`를 `계산`하고, 이 차이를 줄이기 위해 각 `파라미터들을 조정`하는 것이 딥러닝의 전체적인 학습 흐름입니다.

이 차이를 구하는 데 사용되는 함수는 손실함수(Loss function) 또는 비용함수(Cost function)라고 부릅니다

## 예시

1. `평균 제곱 오차(Mean Squared Error, MSE)`: `회귀` 문제에서 주로 사용되는 손실 함수입니다. 실제 값과 예측 값의 차이를 제곱한 후 평균을 구합니다. 이 함수는 모델이 예측 결과와 실제 값 사이의 거리를 최소화하도록 학습됩니다.

2. `평균 절대 오차(Mean Absolute Error, MAE)`: `회귀` 문제에서 사용할 수 있는 또 다른 손실 함수입니다. 실제 값과 예측 값의 절대 차이를 평균합니다. 이 함수는 예측 결과와 실제 값 사이의 절대 거리를 최소화하려고 시도하지만, 큰 오차 값을 가질 때보다 작은 오차 값을 더 크게 처리한다는 특징이 있습니다.

3. `교차 엔트로피 오차(Cross-Entropy Loss)`: `분류` 문제에서 주로 사용되는 손실 함수입니다. 실제 클래스의 확률 분포와 예측된 확률 분포 사이의 차이를 측정합니다. 모델이 정확한 분류에 대해 높은 확률 값을 할당하고, 잘못된 분류에 대해 낮은 확률 값을 할당하도록 학습됩니다.

4. `이진 교차 엔트로피 오차(Binary Cross-Entropy Loss)`: `이진 분류` 문제에서 사용되는 손실 함수로, 두 개의 클래스가 있는 경우의 교차 엔트로피입니다. 대표적인 예로 스팸 메일 분류와 같은 이진 분류 문제에서 사용됩니다.

5. `멀티 클래스 교차 엔트로피 오차(Multiclass Cross-Entropy Loss)`: `다중 클래스 분류` 문제에서 사용되는 손실 함수로, 여러 개의 클래스가 있는 경우의 교차 엔트로피입니다. 대표적인 예로 손글씨 숫자 인식과 같은 다중 클래스 분류 문제에서 사용됩니다.

6. `Hinge Loss`: `서포트 벡터 머신(SVM)을 기반`으로 하는 딥러닝 모델에서 사용되는 손실 함수로, 마진을 최대화하는 방식으로 학습됩니다. 주로 이진 분류 문제에서 사용되며, 마진을 최대화함으로써 두 클래스 사이의 경계를 찾는 데 도움을 줍니다.

7. `주식 회귀 오차(Huber Loss)`: `회귀` 문제에서 약간의 이상치에 덜 민감하게 만들어진 손실 함수입니다. 평균 제곱 오차와 평균 절대 오차를 함쳐 장점을 취한 함수로, 오차가 작을 경우에는 평균 제곱 오차와 비슷하게 작동하고, 오차가 클 때는 평균 절대 오차와 비슷하게 작동합니다.

각 손실 함수는 특정 태스크에 적합한 특성을 가지고 있어, 해당 태스크의 목표와 데이터의 특성에 따라 적절한 손실 함수를 선택하는 것이 중요합니다.