# 경사하강법

경사 하강법(Gradient Descent)은 딥러닝 및 다양한 최적화 문제에서 `손실 함수`를 최소화하는 파라미터(가중치와 편향)를 찾기 위해 사용되는 반복 알고리즘입니다. 경사 하강법은 손실 함수의 기울기(그래디언트)를 계산하여 기울기가 가리키는 방향으로 파라미터를 조금씩 업데이트하여 손실 함수의 값을 점차 줄여 나갑니다.

다음은 경사 하강법의 기본적인 과정입니다.

1. `초기 파라미터 설정`: 파라미터(가중치와 편향)에 초기값을 설정합니다. 이 초기값은 랜덤하게 선택되거나 사전에 정의된 값으로 설정할 수 있습니다.

2. `그래디언트 계산`: 현재 파라미터에서 손실 함수의 그래디언트(기울기)를 계산합니다. 이 그래디언트는 손실 함수의 국부적인 최솟값을 찾기 위해 이동해야 하는 방향을 나타냅니다.

3. `파라미터 업데이트`: 그래디언트와 **학습률(learning rate)**라는 하이퍼파라미터를 곱한 값을 현재 파라미터에서 빼고 다음 값을 계산합니다. 학습률은 파라미터가 얼마나 빠르게 업데이트되는지를 제어하는 요소로, 값이 작을수록 느리게 학습하고 값이 클수록 빠르게 학습합니다.

4. `수렴 또는 반복` 횟수에 도달할 때까지 2번과 3번 과정을 반복합니다.

경사 하강법은 전체 데이터셋을 사용하여 그래디언트를 계산하는 배치 경사 하강법과, 데이터셋의 일부(미니배치)를 사용하여 그래디언트를 계산하는 미니배치 경사 하강법, 그리고 매번 하나의 데이터를 사용하여 그래디언트를 계산하는 확률적 경사 하강법(Stochastic Gradient Descent, SGD) 등 다양한 변형 형태가 있습니다. 각각은 빠른 학습 속도와 정확한 수렴 간의 균형이 다르므로, 문제와 데이터의 특성에 따라 적절한 방법을 선택해야 합니다.