# 기울기 소실 문제(Gradient Vanishing Problem)

기울기 소실 문제(Gradient Vanishing Problem)란 딥러닝 모델에서 역전파(backpropagation) 과정 중에, 가중치 값을 업데이트 하기 위해 계산되는 기울기(그래디언트) 값이 점차 작아져서 매우 작은 값에 근접해버리는 현상입니다. 이 문제가 발생하면 딥러닝 모델의 학습에 다음과 같은 현상이 나타납니다:


1. 초기 레이어의 가중치가 거의 업데이트되지 않음: 기울기 값이 작아 지면서 가중치는 매우 미세한 값만큼만 업데이트 되게 됩니다. 이는 특히 모델의 입력 레이어와 가까운 초기 레이어에서 두드러집니다. 따라서 초기 레이어의 가중치는 학습 과정에서 거의 변하지 않고 초기화된 상태를 유지하게 됩니다.

2. 학습 속도 저하: 초기 레이어의 가중치가 제대로 업데이트되지 않으면 해당 레이어가 학습할 패턴을 충분히 표현하지 못하게 됩니다. 이로 인해 전체 모델의 학습에 영향을 미치며, 학습 속도가 저하되고 최적의 성능에 다다르지 못할 수 있습니다.

3. 모델 정확도 저하: 학습 속도 저하와 함께 학습이 미진한 초기 레이어로 인해 전체 모델의 정확도 또한 저하되므로, 모델 성능에 악영향을 미칩니다.  


기울기 소실 문제를 `완화하기 위한 다양한 방법들`이 있습니다. 여기에는 은닉층 활성화 함수로 ReLU 또는 Leaky ReLU를 사용하거나, LSTM(Long Short-Term Memory)이나 GRU(Gated Recurrent Unit)과 같은 고급 순환 레이어를 사용하는 것 등이 포함됩니다. 또한, 모델 학습 초기의 가중치 초기화 방법을 변경하거나 배치 정규화(Batch Normalization) 기법을 사용하는 것 등이 방법론적인 접근 방식 중 일부입니다.