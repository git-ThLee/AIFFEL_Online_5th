# 임베딩(Embeding)

## 희소 표현(Sparse Representation)

> 벡터의 특정 차원에 단어 혹은 의미를 직접 매핑하는 방식

대부분의 값이 0으로 이루어진 벡터 또는 행렬을 의미합니다. 즉, ``해당 데이터에서 실제로 중요한 정보가 있는 일부 요소만을 표현하고 나머지 요소는 0으로 표현하는 것``입니다. 이는 대부분의 자연어 처리 문제에서 매우 일반적으로 나타나는 현상입니다.

### 예시

 사과: [ 0, 0 ] , 바나나: [ 1, 1 ] , 배: [ 0, 1 ] 정도로 표현할 수 있겠네요. 첫 번째 요소는 모양(0:둥글다, 1:길쭉하다)을 나타내고, 두 번째 요소는 색상(0:빨강, 1:노랑)을 나타내는 거죠! 배는 모양 기준으로는 사과와 가깝고, 색상 기준으로는 바나나와 가깝다는 것이 아주 잘 표현되고 있습니다. 😎  

> 단점 : 모든 단어를 표현하게 되면 몇 차원의 벡터가 필요할지 알 수 없음. 즉, 비용적 문제 발생

## 분산 표현(Distributed Representation)

> 모든 단어를 고정 차원 (예를 들어 256차원)의 벡터로 표현하는 것

### 예시

- 나는 밥을 먹는다.
- 나는 떡을 먹는다.
- 나는 _____을 먹는다.

`나는` 과 `먹는다` 사이에 주로 나타나는 것들 사이에 어떤 의미적인 유사점이 있죠? 유사한 맥락에 나타난 단어들끼리는 두 단어 벡터 사이의 거리를 가깝게 하고, 그렇지 않은 단어들끼리는 멀어지도록 조금씩 조정해 주는 것뿐입니다. 이런 방식으로 얻어지는 단어 벡터를 단어의 분산 표현(Distributed Representation) 이라고 합니다

`분산 표현을 사용하면 희소 표현과는 다르게 단어 간의 유사도 를 계산으로 구할 수 있다는 장점이 있습니다!`


## Embedding 레이어

Embedding 레이어는 아주 쉬운데, 간단하게 말하면 컴퓨터용 단어 사전입니다. 컴퓨터는 알아서 사전을 만들고, 수많은 데이터를 거치며 각 단어의 의미(분산 표현)를 차근차근 업데이트합니다.

![image](https://d3s0tskafalll9.cloudfront.net/media/images/F-24-12.max-800x600.png)  

Embedding 레이어는 입력으로 들어온 단어를 분산 표현으로 연결해 주는 역할을 하는데 그것이 Weight에서 특정 행을 읽어오는 것과 같아 이 레이어를 룩업 테이블(Lookup Table) 이라고 부르기도 합니다  

`단어가 룩업 테이블에 매핑되는 부분! 어떤 원리로 동작하는 걸까요?`  

- 원-핫 인코딩(One-Hot Encoding)
- 단어 임베딩(Word Embedding)

[인코딩,임베딩 참고 블로그](https://brunch.co.kr/@kakao-it/189)  

> Q. 인간, 펭귄, 오리, 문어, 사람을 순서대로 원-핫 인코딩을 사용해서 표현하면 오리는 어떻게 표현되나요?  

정답. [0,0,1,0,0]  

> Q. 원-핫 벡터로 표현된 서로 다른 두 단어가 항상 `직교`한다는 것은 무엇을 의미하나요?    

정답. 두 단어가 서로 `독립적`이라는 것을 의미합니다.  

  
Embedding 레이어는 아주 쉽지만 주의사항이 하나 있습니다. 기본적으로 딥러닝은 미분을 기반으로 동작하는데, Embedding 레이어는 그저 단어를 대응 시켜 줄 뿐이니 미분이 불가능합니다. 따라서 신경망 설계를 할 때, 어떤 **연산 결과를 Embedding 레이어에 연결시키는 것은 불가능**합니다. 정확히 이해하고자 하면 어려우니, 지금은 주의사항을 숙지하는 것만으로 충분합니다. Embedding 레이어는 **입력에 직접 연결되게 사용**해야 한다는 것을 꼭 기억해 주세요! 그리고 그 입력은 원-핫 인코딩된 단어 벡터의 형태일 때가 이상적입니다.

> Q. 임베딩 레이어의 깊이는 깊을수록 좋을까요?

정답. 임베딩 레이어의 깊이가 깊을수록 항상 좋은 것은 아닙니다. 임베딩 레이어는 일반적으로 입력 데이터의 차원을 줄이는 역할을 하기 때문에, 데이터의 추상화 및 표현 능력을 향상시키는 데 도움을 줄 수 있습니다. 따라서, 적절한 깊이를 선택하는 것이 중요합니다.

하지만 임베딩 레이어의 깊이가 지나치게 깊으면, 모델의 복잡도가 높아져 과적합(Overfitting)이 발생할 가능성이 있습니다. 또한, 임베딩 레이어가 깊을수록 계산 비용이 높아질 수 있습니다. 따라서, 깊이를 선택할 때는 모델의 복잡도와 계산 비용을 고려해야 합니다.

또한, 임베딩 레이어의 깊이는 문제의 복잡도에 따라 달라질 수 있습니다. 간단한 문제에서는 얕은 임베딩 레이어가 충분할 수 있지만, 복잡한 문제에서는 깊은 임베딩 레이어가 필요할 수 있습니다. 따라서, 문제에 맞는 적절한 깊이를 선택하는 것이 중요합니다.

> Q. RNN과 LSTM의 차이점은 무엇인가요?

정답. RNN(Recurrent Neural Network)과 LSTM(Long Short-Term Memory)은 모두 시퀀스 데이터를 처리하기 위한 딥러닝 모델 중 하나입니다. 하지만 LSTM은 RNN의 한계를 보완하기 위해 제안된 모델로, RNN의 기본적인 아이디어를 확장하고 개선한 모델입니다.

RNN은 시퀀스 데이터를 처리하기 위해 고안된 모델로, 이전 시점의 입력 데이터를 현재 시점에 반영하는 재귀적인 구조를 가지고 있습니다. 하지만 RNN은 입력 데이터가 길어질수록 장기 의존성(Long-term Dependency) 문제가 발생하여 이전의 정보가 현재의 정보에 충분히 반영되지 않는 문제가 있습니다.

반면 LSTM은 이러한 RNN의 한계를 보완하기 위해 고안된 모델로, 셀 상태(Cell State)와 게이트(Gate) 메커니즘을 도입하여 장기 의존성 문제를 해결하였습니다. LSTM은 셀 상태를 유지하면서 필요한 정보를 선택적으로 추가하거나 삭제하여 장기 의존성 문제를 해결합니다. 이러한 게이트 메커니즘은 입력 게이트(Input Gate), 삭제 게이트(Forget Gate), 출력 게이트(Output Gate)로 구성되어 있습니다.

따라서, RNN은 기본적으로 이전 시점의 정보를 현재 시점에 반영하는 방식으로 시퀀스 데이터를 처리하지만, LSTM은 셀 상태와 게이트 메커니즘을 도입하여 장기 의존성 문제를 해결한 모델입니다. LSTM은 RNN보다 장기 의존성을 처리하는 능력이 뛰어나지만, 모델 구조가 복잡하고 학습에 필요한 계산 비용이 높은 단점이 있습니다.






