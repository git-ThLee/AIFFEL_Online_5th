# 활성화 함수(Activation Functions)

딥러닝에서는 이 `활성화 함수의 존재가 필수적`입니다. 수학적인 이유가 있지만, 간단히만 설명하자면 이 활성화 함수는 보통 `비선형 함수`를 사용하는데 이 비선형 함수를 MLP 안에 포함시키면서 모델의 표현력이 좋아지게 됩니다.  

(정확히는 레이어 사이에 이 비선형 함수가 포함되지 않은 MLP는 한 개의 레이어로 이루어진 모델과 수학적으로 다른 점이 없습니다.)

## 선형 함수란

대표적인 예로 `y = ax + b와 같은 일차 함수`가 있습니다. 선형 함수의 주요 특징은 입력 값에 대해 비례 관계를 보이며, 함수 값이 직선 형태로 일정하게 증가하거나 감소합니다.

## 선형 함수를 사용의 한계

1. `레이어의 수를 늘려도 복잡한 기능을 표현할 수 없음`: 선형 변환으로만 구성된 딥러닝 모델은 복잡한 데이터 및 패턴을 충분히 표현하지 못하게 됩니다. 이를 수학적으로 증명하면, 선형 함수를 여러 번 중첩하여 적용하더라도 그 결과는 결국 선형 함수로 압축이 가능하며, 이는 복잡한 함수를 표현하는데 한계가 있음을 의미합니다.

2. `기울기 소실 문제 발생`: 선형 함수를 사용하게 되면 `역전파` 과정에서 레이어를 거칠 때마다 기울기가 누적되거나 소실되는 문제가 발생할 가능성이 높아집니다. 이는 모델의 학습속도가 저하되거나 비효율적으로 된다는 것을 의미합니다.


## 비선형 함수란

`직선이 아닌 곡선의 형태를 띄는 함수`입니다. 즉, 입력 값에 대해 출력 값이 일정한 비율로 변하지 않습니다. 비선형 활성화 함수는 딥러닝 모델의 표현력을 강화하는 역할을 수행하며, 선형 함수만으로는 구현할 수 없는 복잡한 패턴과 데이터를 학습하는 데 도움을 줍니다. 예로는 시그모이드 함수, 지수 함수, 로그 함수, ReLU 함수 등이 있습니다.

### 비선형 함수 예시

1. `시그모이드(Sigmoid)` 함수: 시그모이드 함수는 S-형 곡선의 비선형 함수로, `출력 값의 범위를 0과 1 사이`로 제한합니다. 시그모이드 함수는 이진 분류 문제의 출력 레이어에 사용되기도 하며, 확률로 해석될 수 있습니다. 이 함수의 단점은 그래디언트가 큰 값 또는 작은 값에 가까운 경우 기울기 소실 문제가 발생할 수 있다는 것입니다. 주로 `이진 분류 문제에서 사용`되며, 출력값은 확률로 해석될 수 있습니다. 예를 들어, 스팸 메일 필터링이나 댓글 감성 분류와 같은 이진 분류(긍정/부정 또는 스팸/논스팸) 문제에 사용됩니다.

2. `하이퍼볼릭 탄젠트(tanh)` 함수: 하이퍼볼릭 탄젠트 함수는 시그모이드 함수와 비슷한 S-형 곡선이지만 `출력 값의 범위가 -1과 1 사이`입니다. 이로 인해 출력 값의 중심이 0으로 조정되므로, 시그모이드 함수보다 학습에 더 유용할 때가 있습니다. **지금도 기울기 소실 문제가 발생할 가능성이 있습니다**. 각종 `분류 및 회귀 문제에서 사용`되며, 입력값의 범위를 -1과 1 사이로 조정하므로, 시그모이드 함수보다 간혹 학습에서 더 유리할 때가 있습니다. 예를 들어, 시계열 예측이나 텍스트 분류 문제의 은닉층에서 사용됩니다.

3. `ReLU(Rectified Linear Unit)` 함수: ReLU 함수는 입력 값이 `0보다 크면 그대로 출력`하고, `0 이하인 경우에는 0으로 출력`하는 비선형 함수입니다. ReLU 함수는 **기울기 소실 문제를 어느 정도 해결**해주며, 계산이 간단하여 학습 속도를 빠르게 하는 데 도움이 됩니다. 단점으로는 입력 값이 0 이하인 경우 그래디언트가 항상 0이 되어 가중치가 업데이트되지 않는 "죽은 ReLU" 문제가 발생할 수 있습니다. 이미지 인식, 객체 탐지, 음성 인식 등 `컴퓨터 비전 문제`와 `자연어 처리 문제`에 광범위하게 사용됩니다. ReLU 함수는 계산이 간단하고 학습 속도가 빠르므로 오늘날 `딥러닝 모델에서 가장 보편적`으로 사용되는 활성화 함수 중 하나입니다.

4. `Leaky ReLU` 함수: Leaky ReLU는 ReLU 함수의 변형으로, `입력 값이 0 이하인 경우에도 작은 기울기 값을 가지는` 함수입니다. 이로 인해 "죽은 ReLU" 문제를 줄일 수 있습니다. ReLU 함수와 유사한 특성을 가지고 있지만, "죽은 ReLU" 문제를 해결할 수 있는 장점이 있습니다. 이 또한 이미지 인식, 객체 탐지, 음성 인식 등의 컴퓨터 비전 문제와 자연어 처리 문제에 사용됩니다.

5. `ELU(Exponential Linear Unit)` 함수: ELU 함수는 적응 지수를 사용하여 `입력 값이 0 이하인 경우 기울기 값을 부드럽게 조정하는 함수`입니다. 이 함수는 입력 값이 음수일 때도 기울기 소실 문제를 완화하고, 노이즈에 영향을 받지 않으며, 학습 속도를 높이는 데 도움이 됩니다. 기울기 소실 및 노이즈 문제를 해결하는 데 도움이 되므로, `컴퓨터 비전 문제`(이미지 인식, 객체 탐지)와 `자연어 처리 문제`(텍스트 분류, 감성 분석)에 사용됩니다.


