{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44af296c",
   "metadata": {},
   "source": [
    "# 평가 문항\n",
    "\n",
    "|평가문항\t|상세기준|\n",
    "|:---|:---|\n",
    "|1. 분류 모델의 accuracy가 기준 이상 높게 나왔는가? |\t3가지 단어 개수에 대해 8가지 머신러닝 기법을 적용하여 그중 최적의 솔루션을 도출하였다.|\n",
    "|2. 분류 모델의 F1 score가 기준 이상 높게 나왔는가? |\tVocabulary size에 따른 각 머신러닝 모델의 성능변화 추이를 살피고, 해당 머신러닝 알고리즘의 특성에 근거해 원인을 분석하였다.|\n",
    "|3. 딥러닝 모델을 활용해 성능이 비교 및 확인되었는가? |\t동일한 데이터셋과 전처리 조건으로 딥러닝 모델의 성능과 비교하여 결과에 따른 원인을 분석하였다.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ba1295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score #정확도 계산\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5da4f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 정보\n",
      "--------------------------------------------------\n",
      "x_train : (8982,)\n",
      "y_train : (8982,)\n",
      "x_test : (2246,)\n",
      "y_test : (2246,)\n",
      "--------------------------------------------------\n",
      "클래스의 수 : 46\n",
      "--------------------------------------------------\n",
      "훈련용 뉴스의 최대 길이 :2376\n",
      "훈련용 뉴스의 평균 길이 :145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('데이터 정보')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)\n",
    "print('-'*50)\n",
    "print('x_train :',x_train.shape)\n",
    "print('y_train :',y_train.shape)\n",
    "print('x_test :',x_test.shape)\n",
    "print('y_test :' ,y_test.shape)\n",
    "print('-'*50)\n",
    "num_classes = max(y_train) + 1\n",
    "print('클래스의 수 : {}'.format(num_classes))\n",
    "print('-'*50)\n",
    "print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train)))\n",
    "print('훈련용 뉴스의 평균 길이 :{}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "# Plot the histogram of sample lengths\n",
    "axes[0].hist([len(s) for s in x_train], bins=50)\n",
    "axes[0].set_xlabel('length of samples')\n",
    "axes[0].set_ylabel('number of samples')\n",
    "axes[0].set_title('Sample Length Distribution')\n",
    "\n",
    "# Plot the count of each class\n",
    "sns.countplot(x=y_train, ax=axes[1])\n",
    "axes[1].set_title('Class Distribution')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "881e59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(x_train, x_test):\n",
    "    word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "    index_to_word = { index+3 : word for word, index in word_index.items() }\n",
    "    # index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "    for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "      index_to_word[index]=token\n",
    "    # Decode\n",
    "    decoded = []\n",
    "    for i in range(len(x_train)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "        decoded.append(t)\n",
    "\n",
    "    x_train = decoded\n",
    "    decoded_test = []\n",
    "    for i in range(len(x_test)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "        decoded_test.append(t)\n",
    "\n",
    "    x_test = decoded_test\n",
    "    # DTM\n",
    "    dtmvector = CountVectorizer()\n",
    "    x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "    x_test_dtm = dtmvector.transform(x_test)\n",
    "    # TF-IDF\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    x_train = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "    x_test = tfidf_transformer.transform(x_test_dtm)\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61272159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(num_words = 10000,    # 단어장 개수\n",
    "            max_len_text = None,  # 데이터 길이 조정\n",
    "            max_len_pad = 500,    # pad 길이\n",
    "            vec_type= None,      # 벡터화 타입\n",
    "            class_weight_bool = False  # class_weight 사용 여부\n",
    "           ):\n",
    "    # Data Load\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(\n",
    "                                                            num_words=num_words, \n",
    "                                                            test_split=0.2\n",
    "                                                        )\n",
    "    \n",
    "    # 데이터 길이 제한\n",
    "    if max_len_text:\n",
    "        x_train = [x[:max_len_text] for x in x_train]\n",
    "        x_test = [x[:max_len_text] for x in x_test]\n",
    "        \n",
    "    # tf_idf 방식으로 텍스트를 벡터화\n",
    "    if vec_type == 'tfidf' :\n",
    "        x_train, x_test = tf_idf(x_train, x_test)\n",
    "    else:\n",
    "        # Padding - TF-IDF 에서는 필요없음.\n",
    "        x_train = pad_sequences(x_train, maxlen=max_len_pad)\n",
    "        x_test = pad_sequences(x_test, maxlen=max_len_pad)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Class_weight\n",
    "    if class_weight_bool:\n",
    "        class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "        rf = RandomForestClassifier(n_estimators=5, random_state=0, class_weight=class_weight_dict)\n",
    "    else:\n",
    "        rf = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "        \n",
    "    # Models\n",
    "    models = {\n",
    "        'MultinomialNB' : MultinomialNB(),\n",
    "        'ComplementNB' : ComplementNB(),\n",
    "        'DecisionTreeClassifier' : DecisionTreeClassifier(max_depth=10, random_state=0),\n",
    "        'RandomForestClassifier' : rf,\n",
    "        'VotingClassifier':VotingClassifier(estimators=[ \n",
    "            ('mnb', MultinomialNB()),\n",
    "            ('cnb', ComplementNB()),\n",
    "            ('dt', DecisionTreeClassifier(max_depth=10, random_state=0)),\n",
    "            ('rf', rf)\n",
    "        ], voting='soft'),\n",
    "#         'LogisticRegression' : LogisticRegression(C=10000, penalty='l2', max_iter=3000, random_state=0), # 10분 이상\n",
    "#         'LinearSVC' : LinearSVC(C=1000, penalty='l1', dual=False, max_iter=3000, random_state=0), # 10분 이상\n",
    "#         'GradientBoostingClassifier' : GradientBoostingClassifier(random_state=0), # 15분 이상\n",
    "        }\n",
    "    \n",
    "    accuracys = list()\n",
    "    for idx, (name, model) in enumerate(models.items()):\n",
    "        print(f\"{idx}.모델 : {name}\") \n",
    "        model.fit(x_train, y_train)\n",
    "        predicted = model.predict(x_test) #테스트 데이터에 대한 예측\n",
    "        print(f\"\\t정확도: {accuracy_score(y_test, predicted)}\") \n",
    "        accuracys.append(accuracy_score(y_test, predicted))\n",
    "        print(classification_report(y_test, predicted, zero_division=0))\n",
    "        print('-'*50)\n",
    "    print(f'평균 Acc : {sum(accuracys)/len(accuracys)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a6b2c38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.모델 : MultinomialNB\n",
      "\t정확도: 0.2591273374888691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.08      0.07        12\n",
      "           1       0.00      0.00      0.00       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.53      0.67      0.59       813\n",
      "           4       0.00      0.00      0.00       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.12      0.33      0.18         3\n",
      "           8       1.00      0.05      0.10        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.08      0.11      0.09        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.01      0.03      0.02        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.14      0.03      0.05        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.67      0.02      0.03       133\n",
      "          20       0.08      0.03      0.04        70\n",
      "          21       0.04      0.15      0.07        27\n",
      "          22       0.10      0.29      0.15         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.02      0.06      0.03        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.50      0.10      0.17        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.14      0.14      0.14         7\n",
      "          35       0.10      0.17      0.12         6\n",
      "          36       0.67      0.18      0.29        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.50      0.33      0.40         6\n",
      "          44       0.05      0.80      0.10         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.26      2246\n",
      "   macro avg       0.10      0.08      0.06      2246\n",
      "weighted avg       0.27      0.26      0.23      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "1.모델 : ComplementNB\n",
      "\t정확도: 0.3597506678539626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.00      0.00      0.00       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.45      0.89      0.59       813\n",
      "           4       0.24      0.08      0.12       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.11      0.49      0.18        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.03      0.03      0.03        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.12      0.05      0.07        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.38      0.02      0.04       133\n",
      "          20       0.00      0.00      0.00        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.36      2246\n",
      "   macro avg       0.03      0.03      0.02      2246\n",
      "weighted avg       0.24      0.36      0.25      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "2.모델 : DecisionTreeClassifier\n",
      "\t정확도: 0.40605520926090827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.08      0.08      0.08       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.55      0.79      0.64       813\n",
      "           4       0.32      0.51      0.39       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.29      0.14      0.19        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.31      0.11      0.16        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.10      0.04      0.05        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.14      0.03      0.05        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.09      0.03      0.04        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.10      0.06      0.08       133\n",
      "          20       0.14      0.01      0.03        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.20      0.10      0.13        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.41      2246\n",
      "   macro avg       0.06      0.04      0.04      2246\n",
      "weighted avg       0.30      0.41      0.33      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "3.모델 : RandomForestClassifier\n",
      "\t정확도: 0.43455031166518254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.08      0.04        12\n",
      "           1       0.11      0.21      0.14       105\n",
      "           2       0.04      0.05      0.05        20\n",
      "           3       0.61      0.79      0.69       813\n",
      "           4       0.38      0.45      0.41       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.43      0.21      0.29        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.43      0.32      0.36        38\n",
      "           9       0.10      0.04      0.06        25\n",
      "          10       0.09      0.03      0.05        30\n",
      "          11       0.27      0.24      0.26        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.15      0.05      0.08        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.08      0.05      0.06        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.32      0.20      0.25       133\n",
      "          20       0.31      0.13      0.18        70\n",
      "          21       0.38      0.11      0.17        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.10      0.18        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.20      0.33         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.50      0.17      0.25         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.43      2246\n",
      "   macro avg       0.26      0.12      0.14      2246\n",
      "weighted avg       0.40      0.43      0.40      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "4.모델 : VotingClassifier\n",
      "\t정확도: 0.3695458593054319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.08      0.13        12\n",
      "           1       0.33      0.02      0.04       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.46      0.88      0.60       813\n",
      "           4       0.24      0.10      0.14       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       1.00      0.13      0.23        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.11      0.40      0.17        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.13      0.06      0.08        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.40      0.03      0.06       133\n",
      "          20       0.09      0.01      0.02        70\n",
      "          21       0.11      0.04      0.06        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.20      0.03      0.06        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.50      0.14      0.22         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       1.00      0.09      0.17        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.50      0.17      0.25         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.37      2246\n",
      "   macro avg       0.20      0.08      0.09      2246\n",
      "weighted avg       0.31      0.37      0.28      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "평균 Acc : 0.3658058771148709\n"
     ]
    }
   ],
   "source": [
    "process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ba337",
   "metadata": {},
   "source": [
    "## Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a453977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.모델 : MultinomialNB\n",
      "\t정확도: 0.2591273374888691\n",
      "1.모델 : ComplementNB\n",
      "\t정확도: 0.3597506678539626\n",
      "2.모델 : DecisionTreeClassifier\n",
      "\t정확도: 0.40605520926090827\n",
      "3.모델 : RandomForestClassifier\n",
      "\t정확도: 0.43455031166518254\n",
      "4.모델 : VotingClassifier\n",
      "\t정확도: 0.3695458593054319\n",
      "평균 Acc : 0.3658058771148709\n"
     ]
    }
   ],
   "source": [
    "process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840184bf",
   "metadata": {},
   "source": [
    "# 텍스트 길이 500 적용 vs 미적용\n",
    "\n",
    "- 속도 차이 : 기존 속도 빠른 모델로는 차이 없음\n",
    "- 성능 차이 : \n",
    "    - 미적용 : 0.6871 \n",
    "    - 적용 : `0.6878` ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba237606",
   "metadata": {},
   "source": [
    "## TF-IDF 적용 vs 미적용 \n",
    "\n",
    "- 결과 \n",
    "    - 미적용 : 0.3658\n",
    "    - 적용 : `0.6871` ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703298d9",
   "metadata": {},
   "source": [
    "## Class_weights 적용 vs 미적용\n",
    "\n",
    "- DecisionTree 모델도 적용이 가능하지만, 적용하면 성능이 0.07로 떨어짐. 그래서 제외함\n",
    "- RF 만 적용 ( 다른 모델은 시간이 오래 걸려서 Pass )\n",
    "\n",
    "- 결과\n",
    "    - 미적용 : 0.6871\n",
    "    - 적용 : `0.6955` ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34738f06",
   "metadata": {},
   "source": [
    "# 평가항목 (1)\n",
    "\n",
    "- 평가문항 : 분류 모델의 accuracy가 기준 이상 높게 나왔는가?\n",
    "- 상세사항 : 3가지 단어 개수에 대해 8가지 머신러닝 기법을 적용하여 그중 최적의 솔루션을 도출하였다\n",
    "\n",
    "- 3가지 단어 개수 : 15000, 10000, 5000, None\n",
    "- 8가지 머신러닝 기법 : 머신러닝 모델 8가지    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f5865",
   "metadata": {},
   "source": [
    "### 평가항목 (1) 결과표\n",
    "\n",
    "- Acc\n",
    "\n",
    "| 단어장 개수 | MultinomialNB | ComplementNB | DecisionTree | RandomForest | Voting | LogisticRegression | LinearSVC | GradientBoosting | 평균 Acc |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|None|0.6006|0.7666|0.6121|0.6821|0.6878|`0.8147`1️⃣|0.7916|0.7635|0.7149 |\n",
    "|15000|0.6357|0.7711|0.6068|0.7012|0.7052|0.8130|0.7885|0.7644|0.7232 |\n",
    "|10000|0.6567|0.7707|0.6179|0.7110|0.7172|0.8107|0.7876|0.7711|0.7304 |\n",
    "|5000|0.6723|0.7711|0.6179|0.7128|0.7395|0.8049|0.7769|0.7649|`0.7325`✅ |\n",
    "\n",
    "- 단어장이 늘어날수록⬆ 성능이 떨어지는 것⬇을 확인할 수 있다\n",
    "- 가장 좋은 모델 & 단어장 개수 : `LogisticRegression ( None )`\n",
    "- 최적의 솔루션 : process(num_words= 10000, vec_type= 'tfidf',class_weight_bool= True, max_len_text=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "904f7710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.모델 : MultinomialNB\n",
      "\t정확도: 0.6006233303650935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.79      0.21      0.33       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.72      0.92      0.81       813\n",
      "           4       0.45      0.96      0.61       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.80      0.29      0.42        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.76      0.19      0.31        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.72      0.59      0.65       133\n",
      "          20       0.00      0.00      0.00        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.60      2246\n",
      "   macro avg       0.09      0.07      0.07      2246\n",
      "weighted avg       0.50      0.60      0.51      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "1.모델 : ComplementNB\n",
      "\t정확도: 0.7666963490650045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.63      0.89      0.74       105\n",
      "           2       0.83      0.50      0.62        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.75      0.93      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.43      0.08      0.13        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.96      0.73      0.83        30\n",
      "          11       0.55      0.67      0.61        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.62      0.54      0.58        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.77      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.65      0.55      0.59        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.85      0.24      0.38        70\n",
      "          21       0.80      0.59      0.68        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.71      0.42      0.53        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.83      0.61      0.70        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.33      0.10      0.15        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.62      0.42      0.47      2246\n",
      "weighted avg       0.75      0.77      0.74      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "2.모델 : DecisionTreeClassifier\n",
      "\t정확도: 0.6121994657168299\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.65      0.41      0.50       105\n",
      "           2       0.86      0.30      0.44        20\n",
      "           3       0.93      0.83      0.88       813\n",
      "           4       0.40      0.89      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.43      0.60        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.50      0.16      0.24        25\n",
      "          10       0.88      0.77      0.82        30\n",
      "          11       0.60      0.61      0.61        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.78      0.67        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.44      0.51       133\n",
      "          20       0.60      0.09      0.15        70\n",
      "          21       0.50      0.04      0.07        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.67      0.06      0.12        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.50      0.10      0.17        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.61      2246\n",
      "   macro avg       0.22      0.13      0.14      2246\n",
      "weighted avg       0.61      0.61      0.57      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "3.모델 : RandomForestClassifier\n",
      "\t정확도: 0.6821015138023152\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.50      0.48        12\n",
      "           1       0.43      0.66      0.52       105\n",
      "           2       0.57      0.65      0.60        20\n",
      "           3       0.80      0.87      0.83       813\n",
      "           4       0.64      0.82      0.72       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.80      0.57      0.67        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.60      0.47      0.53        38\n",
      "           9       0.82      0.56      0.67        25\n",
      "          10       0.72      0.43      0.54        30\n",
      "          11       0.61      0.57      0.59        83\n",
      "          12       0.38      0.23      0.29        13\n",
      "          13       0.56      0.38      0.45        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.65      0.46      0.54        99\n",
      "          17       0.75      0.50      0.60        12\n",
      "          18       0.53      0.45      0.49        20\n",
      "          19       0.61      0.53      0.57       133\n",
      "          20       0.58      0.40      0.47        70\n",
      "          21       0.64      0.33      0.44        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.50      0.25      0.33        12\n",
      "          24       0.38      0.16      0.22        19\n",
      "          25       0.93      0.42      0.58        31\n",
      "          26       0.86      0.75      0.80         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.50      0.15      0.24        13\n",
      "          32       1.00      0.20      0.33        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.38      0.43      0.40         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.27      0.35        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.50      0.25      0.33         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.67      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.68      2246\n",
      "   macro avg       0.57      0.39      0.44      2246\n",
      "weighted avg       0.67      0.68      0.66      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "4.모델 : VotingClassifier\n",
      "\t정확도: 0.6878895814781835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.80      0.60      0.68       105\n",
      "           2       0.88      0.35      0.50        20\n",
      "           3       0.79      0.92      0.85       813\n",
      "           4       0.55      0.94      0.69       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.21      0.35        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.20      0.03      0.05        38\n",
      "           9       1.00      0.40      0.57        25\n",
      "          10       0.88      0.77      0.82        30\n",
      "          11       0.71      0.61      0.66        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.40      0.05      0.10        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.70      0.67      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.67      0.71      0.69       133\n",
      "          20       0.94      0.21      0.35        70\n",
      "          21       1.00      0.07      0.14        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.11      0.19        19\n",
      "          25       0.67      0.06      0.12        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.40      0.57         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.50      0.10      0.17        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       1.00      0.20      0.33         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.69      2246\n",
      "   macro avg       0.47      0.21      0.25      2246\n",
      "weighted avg       0.66      0.69      0.63      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "5.모델 : LogisticRegression\n",
      "\t정확도: 0.8147818343722173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.78      0.76       105\n",
      "           2       0.79      0.75      0.77        20\n",
      "           3       0.92      0.93      0.92       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.71      0.71      0.71        38\n",
      "           9       0.85      0.88      0.86        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.67      0.73      0.70        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.64      0.62      0.63        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.80      0.44      0.57         9\n",
      "          16       0.71      0.78      0.74        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.81      0.65      0.72        20\n",
      "          19       0.68      0.71      0.70       133\n",
      "          20       0.60      0.50      0.55        70\n",
      "          21       0.69      0.81      0.75        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.67      0.53      0.59        19\n",
      "          25       0.89      0.77      0.83        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.30      0.37        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       1.00      0.58      0.74        12\n",
      "          31       0.78      0.54      0.64        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.44      0.36      0.40        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.78      0.64      0.67      2246\n",
      "weighted avg       0.82      0.81      0.81      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "6.모델 : LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t정확도: 0.7916295636687445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67        12\n",
      "           1       0.74      0.71      0.72       105\n",
      "           2       0.71      0.85      0.77        20\n",
      "           3       0.91      0.92      0.92       813\n",
      "           4       0.82      0.85      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      1.00      0.97        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.63      0.71      0.67        38\n",
      "           9       0.84      0.84      0.84        25\n",
      "          10       0.93      0.83      0.88        30\n",
      "          11       0.64      0.73      0.69        83\n",
      "          12       0.27      0.31      0.29        13\n",
      "          13       0.55      0.49      0.51        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.86      0.67      0.75         9\n",
      "          16       0.65      0.75      0.69        99\n",
      "          17       0.75      0.50      0.60        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.61      0.67      0.64       133\n",
      "          20       0.59      0.47      0.52        70\n",
      "          21       0.56      0.81      0.67        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.64      0.58      0.61        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.79      0.61      0.69        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.80      0.40      0.53        10\n",
      "          29       0.43      0.75      0.55         4\n",
      "          30       1.00      0.58      0.74        12\n",
      "          31       0.88      0.54      0.67        13\n",
      "          32       0.89      0.80      0.84        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.45      0.45      0.45        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.67      0.40      0.50         5\n",
      "          40       0.50      0.30      0.37        10\n",
      "          41       0.57      0.50      0.53         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.79      2246\n",
      "   macro avg       0.71      0.61      0.64      2246\n",
      "weighted avg       0.79      0.79      0.79      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "7.모델 : GradientBoostingClassifier\n",
      "\t정확도: 0.763579697239537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.79      0.70      0.74       105\n",
      "           2       0.65      0.65      0.65        20\n",
      "           3       0.89      0.91      0.90       813\n",
      "           4       0.75      0.81      0.78       474\n",
      "           5       0.25      0.20      0.22         5\n",
      "           6       0.83      0.71      0.77        14\n",
      "           7       0.33      0.33      0.33         3\n",
      "           8       0.59      0.61      0.60        38\n",
      "           9       0.91      0.80      0.85        25\n",
      "          10       0.73      0.80      0.76        30\n",
      "          11       0.62      0.67      0.64        83\n",
      "          12       0.47      0.54      0.50        13\n",
      "          13       0.59      0.46      0.52        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.33      0.22      0.27         9\n",
      "          16       0.71      0.74      0.72        99\n",
      "          17       0.71      0.42      0.53        12\n",
      "          18       0.67      0.50      0.57        20\n",
      "          19       0.70      0.67      0.68       133\n",
      "          20       0.68      0.46      0.55        70\n",
      "          21       0.70      0.59      0.64        27\n",
      "          22       0.20      0.14      0.17         7\n",
      "          23       0.69      0.75      0.72        12\n",
      "          24       0.59      0.53      0.56        19\n",
      "          25       0.81      0.71      0.76        31\n",
      "          26       0.89      1.00      0.94         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.29      0.20      0.24        10\n",
      "          29       0.30      0.75      0.43         4\n",
      "          30       0.45      0.42      0.43        12\n",
      "          31       0.58      0.54      0.56        13\n",
      "          32       0.90      0.90      0.90        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       0.33      0.17      0.22         6\n",
      "          36       0.54      0.64      0.58        11\n",
      "          37       0.33      0.50      0.40         2\n",
      "          38       0.14      0.33      0.20         3\n",
      "          39       0.67      0.40      0.50         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.50      0.50      0.50         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.30      0.50      0.37         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.60      0.56      0.56      2246\n",
      "weighted avg       0.77      0.76      0.76      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "평균 Acc : 0.7149376669634908\n"
     ]
    }
   ],
   "source": [
    "process(num_words= None, vec_type= 'tfidf',class_weight_bool= True, max_len_text=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ad0d8d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.모델 : MultinomialNB\n",
      "\t정확도: 0.6357969723953696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.76      0.53      0.63       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.77      0.91      0.84       813\n",
      "           4       0.48      0.96      0.64       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.04      0.08        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.69      0.41      0.52        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.76      0.45      0.57        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.64      0.74      0.69       133\n",
      "          20       1.00      0.01      0.03        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.64      2246\n",
      "   macro avg       0.13      0.09      0.09      2246\n",
      "weighted avg       0.56      0.64      0.55      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "1.모델 : ComplementNB\n",
      "\t정확도: 0.7711487088156723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.65      0.90      0.75       105\n",
      "           2       0.83      0.50      0.62        20\n",
      "           3       0.90      0.90      0.90       813\n",
      "           4       0.76      0.92      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.50      0.13      0.21        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.54      0.71      0.61        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.62      0.57      0.59        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.79      0.72        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.57      0.60      0.59        20\n",
      "          19       0.54      0.79      0.64       133\n",
      "          20       0.79      0.27      0.40        70\n",
      "          21       0.77      0.63      0.69        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.71      0.42      0.53        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.85      0.71      0.77        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.25      0.10      0.14        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.23      0.38        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.61      0.43      0.47      2246\n",
      "weighted avg       0.75      0.77      0.74      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "2.모델 : DecisionTreeClassifier\n",
      "\t정확도: 0.6068566340160285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.65      0.33      0.44       105\n",
      "           2       0.60      0.30      0.40        20\n",
      "           3       0.92      0.83      0.87       813\n",
      "           4       0.40      0.89      0.55       474\n",
      "           5       0.17      0.20      0.18         5\n",
      "           6       0.90      0.64      0.75        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.33      0.08      0.13        25\n",
      "          10       0.88      0.77      0.82        30\n",
      "          11       0.58      0.60      0.59        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.80      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.65      0.38      0.48       133\n",
      "          20       0.35      0.10      0.16        70\n",
      "          21       0.33      0.04      0.07        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.14      0.05      0.08        19\n",
      "          25       0.67      0.06      0.12        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.50      0.10      0.17        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.61      2246\n",
      "   macro avg       0.19      0.13      0.14      2246\n",
      "weighted avg       0.59      0.61      0.56      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "3.모델 : RandomForestClassifier\n",
      "\t정확도: 0.701246660730187\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.67      0.64        12\n",
      "           1       0.47      0.75      0.58       105\n",
      "           2       0.67      0.60      0.63        20\n",
      "           3       0.79      0.88      0.83       813\n",
      "           4       0.66      0.81      0.73       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.89      0.57      0.70        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.71      0.58      0.64        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.88      0.70      0.78        30\n",
      "          11       0.65      0.54      0.59        83\n",
      "          12       0.33      0.15      0.21        13\n",
      "          13       0.62      0.49      0.55        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.62      0.63      0.62        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.59      0.50      0.54        20\n",
      "          19       0.66      0.50      0.57       133\n",
      "          20       0.61      0.33      0.43        70\n",
      "          21       0.75      0.44      0.56        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.58      0.58      0.58        12\n",
      "          24       0.80      0.21      0.33        19\n",
      "          25       0.81      0.68      0.74        31\n",
      "          26       1.00      0.38      0.55         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.33      0.25      0.29         4\n",
      "          30       1.00      0.17      0.29        12\n",
      "          31       0.50      0.08      0.13        13\n",
      "          32       1.00      0.20      0.33        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.50      0.29      0.36         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.18      0.24        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.67      0.67      0.67         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.70      2246\n",
      "   macro avg       0.63      0.41      0.46      2246\n",
      "weighted avg       0.70      0.70      0.68      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "4.모델 : VotingClassifier\n",
      "\t정확도: 0.705253784505788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.42      0.59        12\n",
      "           1       0.69      0.69      0.69       105\n",
      "           2       0.69      0.45      0.55        20\n",
      "           3       0.83      0.90      0.87       813\n",
      "           4       0.57      0.95      0.71       474\n",
      "           5       0.33      0.20      0.25         5\n",
      "           6       1.00      0.50      0.67        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.60      0.16      0.25        38\n",
      "           9       1.00      0.68      0.81        25\n",
      "          10       0.88      0.77      0.82        30\n",
      "          11       0.74      0.63      0.68        83\n",
      "          12       0.33      0.08      0.12        13\n",
      "          13       0.56      0.14      0.22        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.70      0.74      0.72        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       1.00      0.05      0.10        20\n",
      "          19       0.62      0.73      0.67       133\n",
      "          20       0.80      0.11      0.20        70\n",
      "          21       0.75      0.11      0.19        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.50      0.08      0.14        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       1.00      0.10      0.18        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       1.00      0.17      0.29        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.40      0.57         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.10      0.18        10\n",
      "          41       0.50      0.12      0.20         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       1.00      0.20      0.33         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.71      2246\n",
      "   macro avg       0.55      0.24      0.29      2246\n",
      "weighted avg       0.71      0.71      0.66      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "5.모델 : LogisticRegression\n",
      "\t정확도: 0.8130008904719501\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.74      0.78      0.76       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.92      0.93      0.92       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.71      0.71      0.71        38\n",
      "           9       0.85      0.88      0.86        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.67      0.72      0.69        83\n",
      "          12       0.62      0.38      0.48        13\n",
      "          13       0.66      0.62      0.64        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.80      0.44      0.57         9\n",
      "          16       0.71      0.77      0.74        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.81      0.65      0.72        20\n",
      "          19       0.69      0.71      0.70       133\n",
      "          20       0.61      0.53      0.56        70\n",
      "          21       0.67      0.81      0.73        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.88      0.74      0.81        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.88      0.58      0.70        12\n",
      "          31       0.78      0.54      0.64        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.56      0.45      0.50        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.79      0.64      0.67      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "6.모델 : LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t정확도: 0.788512911843277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.75      0.75       105\n",
      "           2       0.72      0.65      0.68        20\n",
      "           3       0.91      0.91      0.91       813\n",
      "           4       0.81      0.85      0.83       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.88      0.84      0.86        25\n",
      "          10       0.92      0.80      0.86        30\n",
      "          11       0.67      0.73      0.70        83\n",
      "          12       0.42      0.38      0.40        13\n",
      "          13       0.50      0.57      0.53        37\n",
      "          14       0.33      0.50      0.40         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.63      0.72      0.67        99\n",
      "          17       0.71      0.42      0.53        12\n",
      "          18       0.87      0.65      0.74        20\n",
      "          19       0.62      0.66      0.64       133\n",
      "          20       0.52      0.46      0.48        70\n",
      "          21       0.56      0.81      0.67        27\n",
      "          22       0.33      0.14      0.20         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.67      0.53      0.59        19\n",
      "          25       0.86      0.61      0.72        31\n",
      "          26       0.78      0.88      0.82         8\n",
      "          27       0.50      0.50      0.50         4\n",
      "          28       0.50      0.30      0.37        10\n",
      "          29       0.43      0.75      0.55         4\n",
      "          30       0.80      0.33      0.47        12\n",
      "          31       0.80      0.62      0.70        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.60      0.55      0.57        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       1.00      0.67      0.80         3\n",
      "          39       1.00      0.40      0.57         5\n",
      "          40       0.40      0.20      0.27        10\n",
      "          41       0.67      0.50      0.57         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.79      2246\n",
      "   macro avg       0.73      0.62      0.64      2246\n",
      "weighted avg       0.79      0.79      0.79      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "7.모델 : GradientBoostingClassifier\n",
      "\t정확도: 0.7644701691896705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        12\n",
      "           1       0.76      0.68      0.72       105\n",
      "           2       0.61      0.70      0.65        20\n",
      "           3       0.88      0.91      0.90       813\n",
      "           4       0.76      0.82      0.79       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.79      0.79      0.79        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.62      0.66      0.64        38\n",
      "           9       0.82      0.72      0.77        25\n",
      "          10       0.82      0.77      0.79        30\n",
      "          11       0.61      0.67      0.64        83\n",
      "          12       0.47      0.54      0.50        13\n",
      "          13       0.52      0.41      0.45        37\n",
      "          14       0.12      0.50      0.20         2\n",
      "          15       0.14      0.11      0.12         9\n",
      "          16       0.71      0.73      0.72        99\n",
      "          17       0.64      0.58      0.61        12\n",
      "          18       0.62      0.50      0.56        20\n",
      "          19       0.72      0.61      0.66       133\n",
      "          20       0.65      0.49      0.56        70\n",
      "          21       0.59      0.63      0.61        27\n",
      "          22       0.25      0.14      0.18         7\n",
      "          23       0.64      0.58      0.61        12\n",
      "          24       0.50      0.47      0.49        19\n",
      "          25       0.88      0.68      0.76        31\n",
      "          26       0.89      1.00      0.94         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.33      0.30      0.32        10\n",
      "          29       0.30      0.75      0.43         4\n",
      "          30       0.45      0.42      0.43        12\n",
      "          31       0.62      0.38      0.48        13\n",
      "          32       0.90      0.90      0.90        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.50      0.43      0.46         7\n",
      "          35       0.25      0.17      0.20         6\n",
      "          36       0.80      0.73      0.76        11\n",
      "          37       0.50      1.00      0.67         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.67      0.40      0.50         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.38      0.38      0.38         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.50      0.67      0.57         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.62      0.59      0.59      2246\n",
      "weighted avg       0.76      0.76      0.76      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "평균 Acc : 0.723285841495993\n"
     ]
    }
   ],
   "source": [
    "process(num_words= 15000, vec_type= 'tfidf',class_weight_bool= True, max_len_text=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5521064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.모델 : MultinomialNB\n",
      "\t정확도: 0.6567230632235085\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.61      0.68      0.64       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.81      0.90      0.85       813\n",
      "           4       0.51      0.96      0.67       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.08      0.15        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.66      0.64      0.65        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.03      0.05        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.69      0.56      0.61        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.61      0.78      0.68       133\n",
      "          20       1.00      0.04      0.08        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.03      0.06        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.66      2246\n",
      "   macro avg       0.17      0.10      0.10      2246\n",
      "weighted avg       0.59      0.66      0.58      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "1.모델 : ComplementNB\n",
      "\t정확도: 0.7707034728406055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.64      0.88      0.74       105\n",
      "           2       0.83      0.50      0.62        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.75      0.92      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.50      0.13      0.21        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.55      0.75      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.59      0.59      0.59        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.79      0.72        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.56      0.79      0.65       133\n",
      "          20       0.75      0.30      0.43        70\n",
      "          21       0.75      0.67      0.71        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.75      0.50      0.60        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.85      0.74      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.25      0.10      0.14        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.76      0.77      0.75      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "2.모델 : DecisionTreeClassifier\n",
      "\t정확도: 0.6179875333926982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.74      0.41      0.53       105\n",
      "           2       0.78      0.35      0.48        20\n",
      "           3       0.92      0.81      0.87       813\n",
      "           4       0.40      0.89      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.64      0.78        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.85      0.77      0.81        30\n",
      "          11       0.62      0.52      0.57        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.58      0.81      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.59      0.43      0.50       133\n",
      "          20       0.36      0.06      0.10        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.50      0.16      0.24        19\n",
      "          25       0.60      0.19      0.29        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.22      0.17      0.18      2246\n",
      "weighted avg       0.60      0.62      0.58      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "3.모델 : RandomForestClassifier\n",
      "\t정확도: 0.7110418521816563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64        12\n",
      "           1       0.53      0.76      0.62       105\n",
      "           2       0.57      0.60      0.59        20\n",
      "           3       0.84      0.88      0.86       813\n",
      "           4       0.64      0.85      0.73       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.75      0.64      0.69        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.58      0.58      0.58        38\n",
      "           9       0.73      0.64      0.68        25\n",
      "          10       0.92      0.73      0.81        30\n",
      "          11       0.60      0.57      0.58        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.52      0.41      0.45        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       1.00      0.11      0.20         9\n",
      "          16       0.57      0.59      0.58        99\n",
      "          17       0.33      0.17      0.22        12\n",
      "          18       0.54      0.35      0.42        20\n",
      "          19       0.69      0.58      0.63       133\n",
      "          20       0.61      0.36      0.45        70\n",
      "          21       0.85      0.41      0.55        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.60      0.25      0.35        12\n",
      "          24       0.67      0.32      0.43        19\n",
      "          25       0.64      0.52      0.57        31\n",
      "          26       1.00      0.62      0.77         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       1.00      0.30      0.46        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       1.00      0.25      0.40        12\n",
      "          31       0.25      0.08      0.12        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.29      0.18      0.22        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.50      0.25      0.33         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.50      0.67         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.71      2246\n",
      "   macro avg       0.68      0.42      0.48      2246\n",
      "weighted avg       0.71      0.71      0.70      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "4.모델 : VotingClassifier\n",
      "\t정확도: 0.7172751558325913\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        12\n",
      "           1       0.66      0.68      0.67       105\n",
      "           2       0.83      0.50      0.62        20\n",
      "           3       0.86      0.90      0.88       813\n",
      "           4       0.59      0.93      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.50      0.67        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.64      0.24      0.35        38\n",
      "           9       0.76      0.76      0.76        25\n",
      "          10       0.85      0.77      0.81        30\n",
      "          11       0.65      0.64      0.64        83\n",
      "          12       0.50      0.08      0.13        13\n",
      "          13       0.69      0.24      0.36        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.66      0.76      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.50      0.05      0.09        20\n",
      "          19       0.59      0.77      0.67       133\n",
      "          20       1.00      0.20      0.33        70\n",
      "          21       1.00      0.15      0.26        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.33      0.08      0.13        12\n",
      "          24       0.67      0.21      0.32        19\n",
      "          25       0.70      0.23      0.34        31\n",
      "          26       1.00      0.50      0.67         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       1.00      0.10      0.18        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.72      2246\n",
      "   macro avg       0.51      0.29      0.33      2246\n",
      "weighted avg       0.71      0.72      0.68      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "5.모델 : LogisticRegression\n",
      "\t정확도: 0.8107747105966162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.74      0.78      0.76       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.80      0.88      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.68      0.71      0.69        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.67      0.72      0.69        83\n",
      "          12       0.62      0.38      0.48        13\n",
      "          13       0.65      0.59      0.62        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.80      0.44      0.57         9\n",
      "          16       0.72      0.77      0.74        99\n",
      "          17       0.80      0.67      0.73        12\n",
      "          18       0.87      0.65      0.74        20\n",
      "          19       0.69      0.70      0.69       133\n",
      "          20       0.61      0.50      0.55        70\n",
      "          21       0.69      0.81      0.75        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.88      0.68      0.76        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.40      0.44        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.88      0.58      0.70        12\n",
      "          31       0.78      0.54      0.64        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.50      0.45      0.48        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.60      0.30      0.40        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.75      0.63      0.66      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "6.모델 : LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t정확도: 0.7876224398931434\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        12\n",
      "           1       0.75      0.75      0.75       105\n",
      "           2       0.67      0.70      0.68        20\n",
      "           3       0.91      0.92      0.92       813\n",
      "           4       0.81      0.84      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.80      0.86      0.83        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.63      0.68      0.66        38\n",
      "           9       0.76      0.88      0.81        25\n",
      "          10       0.84      0.70      0.76        30\n",
      "          11       0.66      0.75      0.70        83\n",
      "          12       0.43      0.46      0.44        13\n",
      "          13       0.60      0.57      0.58        37\n",
      "          14       1.00      0.50      0.67         2\n",
      "          15       0.71      0.56      0.63         9\n",
      "          16       0.66      0.74      0.70        99\n",
      "          17       1.00      0.42      0.59        12\n",
      "          18       0.74      0.70      0.72        20\n",
      "          19       0.64      0.64      0.64       133\n",
      "          20       0.55      0.47      0.51        70\n",
      "          21       0.54      0.74      0.62        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.54      0.58      0.56        12\n",
      "          24       0.65      0.58      0.61        19\n",
      "          25       0.89      0.55      0.68        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.57      0.40      0.47        10\n",
      "          29       0.43      0.75      0.55         4\n",
      "          30       0.67      0.33      0.44        12\n",
      "          31       0.67      0.46      0.55        13\n",
      "          32       0.91      1.00      0.95        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.67      0.57      0.62         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.56      0.45      0.50        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.67      0.80         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.33      0.20      0.25        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       0.40      0.67      0.50         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       0.50      0.80      0.62         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.79      2246\n",
      "   macro avg       0.69      0.61      0.62      2246\n",
      "weighted avg       0.79      0.79      0.78      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "7.모델 : GradientBoostingClassifier\n",
      "\t정확도: 0.7711487088156723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       0.78      0.67      0.72       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.88      0.91      0.89       813\n",
      "           4       0.77      0.84      0.80       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.81      0.93      0.87        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.66      0.66      0.66        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.63      0.62        83\n",
      "          12       0.44      0.54      0.48        13\n",
      "          13       0.60      0.49      0.54        37\n",
      "          14       0.14      0.50      0.22         2\n",
      "          15       0.43      0.33      0.38         9\n",
      "          16       0.73      0.73      0.73        99\n",
      "          17       0.69      0.75      0.72        12\n",
      "          18       0.60      0.60      0.60        20\n",
      "          19       0.69      0.65      0.67       133\n",
      "          20       0.67      0.43      0.52        70\n",
      "          21       0.61      0.70      0.66        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.58      0.58      0.58        12\n",
      "          24       0.65      0.58      0.61        19\n",
      "          25       0.88      0.68      0.76        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.50      0.50      0.50         4\n",
      "          28       0.33      0.30      0.32        10\n",
      "          29       0.38      0.75      0.50         4\n",
      "          30       0.45      0.42      0.43        12\n",
      "          31       0.62      0.38      0.48        13\n",
      "          32       0.90      0.90      0.90        10\n",
      "          33       0.75      0.60      0.67         5\n",
      "          34       0.50      0.29      0.36         7\n",
      "          35       0.80      0.67      0.73         6\n",
      "          36       0.55      0.55      0.55        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.20      0.10      0.13        10\n",
      "          41       0.33      0.38      0.35         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.57      0.67      0.62         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.61      0.59      0.59      2246\n",
      "weighted avg       0.77      0.77      0.77      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "평균 Acc : 0.7304096170970614\n"
     ]
    }
   ],
   "source": [
    "process(num_words= 10000, vec_type= 'tfidf',class_weight_bool= True, max_len_text=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0dde521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.모델 : MultinomialNB\n",
      "\t정확도: 0.672306322350846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.51      0.79      0.62       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.85      0.89      0.87       813\n",
      "           4       0.59      0.95      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.48      0.73      0.58        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.14      0.24        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.66      0.62        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.51      0.80      0.62       133\n",
      "          20       0.90      0.13      0.23        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.06      0.12        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.16      0.12      0.11      2246\n",
      "weighted avg       0.60      0.67      0.60      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "1.모델 : ComplementNB\n",
      "\t정확도: 0.7711487088156723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.63      0.87      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.74      0.92      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.57      0.21      0.31        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.55      0.77      0.64        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.69      0.59      0.64        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.67      0.79      0.72        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.56      0.79      0.66       133\n",
      "          20       0.79      0.33      0.46        70\n",
      "          21       0.78      0.67      0.72        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.33      0.44        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.86      0.77      0.81        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.15      0.27        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.76      0.77      0.75      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "2.모델 : DecisionTreeClassifier\n",
      "\t정확도: 0.6179875333926982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.65      0.35      0.46       105\n",
      "           2       0.80      0.40      0.53        20\n",
      "           3       0.93      0.85      0.88       813\n",
      "           4       0.40      0.89      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.43      0.60        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.86      0.96      0.91        25\n",
      "          10       0.89      0.83      0.86        30\n",
      "          11       0.56      0.61      0.59        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.57      0.79      0.66        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.29      0.39       133\n",
      "          20       0.38      0.07      0.12        70\n",
      "          21       0.50      0.04      0.07        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.67      0.06      0.12        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.10      0.18        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.25      0.15      0.16      2246\n",
      "weighted avg       0.61      0.62      0.57      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "3.모델 : RandomForestClassifier\n",
      "\t정확도: 0.7128227960819234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67        12\n",
      "           1       0.50      0.68      0.58       105\n",
      "           2       0.63      0.60      0.62        20\n",
      "           3       0.84      0.87      0.86       813\n",
      "           4       0.67      0.82      0.74       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.75      0.86      0.80        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.61      0.58      0.59        38\n",
      "           9       0.77      0.68      0.72        25\n",
      "          10       0.82      0.77      0.79        30\n",
      "          11       0.60      0.59      0.59        83\n",
      "          12       0.33      0.23      0.27        13\n",
      "          13       0.55      0.46      0.50        37\n",
      "          14       0.33      1.00      0.50         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.58      0.59      0.58        99\n",
      "          17       0.75      0.25      0.38        12\n",
      "          18       0.56      0.50      0.53        20\n",
      "          19       0.65      0.54      0.59       133\n",
      "          20       0.61      0.43      0.50        70\n",
      "          21       0.71      0.56      0.63        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.33      0.17      0.22        12\n",
      "          24       0.71      0.26      0.38        19\n",
      "          25       0.81      0.71      0.76        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.20      0.10      0.13        10\n",
      "          29       0.33      0.25      0.29         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.50      0.15      0.24        13\n",
      "          32       0.89      0.80      0.84        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.27      0.30        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.50      0.25      0.33         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.50      0.67      0.57         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.71      2246\n",
      "   macro avg       0.56      0.44      0.47      2246\n",
      "weighted avg       0.70      0.71      0.70      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "4.모델 : VotingClassifier\n",
      "\t정확도: 0.7395369545859305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.58      0.74        12\n",
      "           1       0.61      0.73      0.67       105\n",
      "           2       0.86      0.60      0.71        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.64      0.93      0.76       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.43      0.57        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.75      0.32      0.44        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.63      0.75      0.69        83\n",
      "          12       0.50      0.08      0.13        13\n",
      "          13       0.71      0.46      0.56        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.64      0.78      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.67      0.30      0.41        20\n",
      "          19       0.55      0.78      0.65       133\n",
      "          20       0.71      0.29      0.41        70\n",
      "          21       0.85      0.41      0.55        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.50      0.05      0.10        19\n",
      "          25       0.75      0.19      0.31        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.40      0.57        10\n",
      "          33       1.00      0.20      0.33         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.67      0.18      0.29        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.74      2246\n",
      "   macro avg       0.49      0.31      0.35      2246\n",
      "weighted avg       0.72      0.74      0.71      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "5.모델 : LogisticRegression\n",
      "\t정확도: 0.804986642920748\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.73      0.77      0.75       105\n",
      "           2       0.70      0.80      0.74        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.81      0.86      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.68      0.71      0.69        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.83      0.88        30\n",
      "          11       0.64      0.73      0.69        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.63      0.65      0.64        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.80      0.44      0.57         9\n",
      "          16       0.69      0.75      0.72        99\n",
      "          17       0.80      0.67      0.73        12\n",
      "          18       0.81      0.65      0.72        20\n",
      "          19       0.68      0.68      0.68       133\n",
      "          20       0.59      0.50      0.54        70\n",
      "          21       0.71      0.81      0.76        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.67      0.53      0.59        19\n",
      "          25       0.87      0.65      0.74        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.40      0.40      0.40        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       1.00      0.50      0.67        12\n",
      "          31       0.80      0.62      0.70        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.43      0.27      0.33        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.60      0.30      0.40        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.76      0.61      0.65      2246\n",
      "weighted avg       0.80      0.80      0.80      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "6.모델 : LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t정확도: 0.7769367764915405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.72      0.71      0.72       105\n",
      "           2       0.79      0.75      0.77        20\n",
      "           3       0.89      0.90      0.90       813\n",
      "           4       0.80      0.84      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.81      0.93      0.87        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.67      0.68      0.68        38\n",
      "           9       0.77      0.80      0.78        25\n",
      "          10       0.88      0.77      0.82        30\n",
      "          11       0.67      0.73      0.70        83\n",
      "          12       0.50      0.38      0.43        13\n",
      "          13       0.53      0.62      0.57        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.65      0.71      0.68        99\n",
      "          17       1.00      0.42      0.59        12\n",
      "          18       0.81      0.65      0.72        20\n",
      "          19       0.65      0.66      0.65       133\n",
      "          20       0.53      0.50      0.51        70\n",
      "          21       0.57      0.78      0.66        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.81      0.55      0.65        31\n",
      "          26       0.78      0.88      0.82         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.43      0.75      0.55         4\n",
      "          30       0.57      0.33      0.42        12\n",
      "          31       0.75      0.46      0.57        13\n",
      "          32       0.89      0.80      0.84        10\n",
      "          33       0.67      0.80      0.73         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       0.67      0.33      0.44         6\n",
      "          36       0.44      0.36      0.40        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.60      0.60      0.60         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.50      0.50      0.50         8\n",
      "          42       0.50      0.33      0.40         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.66      0.61      0.61      2246\n",
      "weighted avg       0.78      0.78      0.77      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "7.모델 : GradientBoostingClassifier\n",
      "\t정확도: 0.7649154051647373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.58      0.64        12\n",
      "           1       0.78      0.66      0.72       105\n",
      "           2       0.68      0.65      0.67        20\n",
      "           3       0.89      0.91      0.90       813\n",
      "           4       0.78      0.83      0.81       474\n",
      "           5       0.17      0.20      0.18         5\n",
      "           6       0.92      0.79      0.85        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.55      0.61      0.57        38\n",
      "           9       0.87      0.80      0.83        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.67      0.67      0.67        83\n",
      "          12       0.29      0.38      0.33        13\n",
      "          13       0.56      0.51      0.54        37\n",
      "          14       0.08      0.50      0.14         2\n",
      "          15       0.33      0.22      0.27         9\n",
      "          16       0.73      0.75      0.74        99\n",
      "          17       0.39      0.58      0.47        12\n",
      "          18       0.62      0.50      0.56        20\n",
      "          19       0.67      0.62      0.65       133\n",
      "          20       0.59      0.46      0.52        70\n",
      "          21       0.65      0.56      0.60        27\n",
      "          22       0.40      0.29      0.33         7\n",
      "          23       0.38      0.42      0.40        12\n",
      "          24       0.76      0.68      0.72        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.40      0.50      0.44         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       0.25      0.50      0.33         4\n",
      "          30       0.43      0.50      0.46        12\n",
      "          31       0.60      0.46      0.52        13\n",
      "          32       0.88      0.70      0.78        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       0.50      0.29      0.36         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.67      0.55      0.60        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.25      0.33      0.29         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.20      0.10      0.13        10\n",
      "          41       0.44      0.50      0.47         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.44      0.67      0.53         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.59      0.57      0.57      2246\n",
      "weighted avg       0.77      0.76      0.76      2246\n",
      "\n",
      "--------------------------------------------------\n",
      "평균 Acc : 0.7325801424755121\n"
     ]
    }
   ],
   "source": [
    "process(num_words= 5000, vec_type= 'tfidf',class_weight_bool= True, max_len_text=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec273db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc5b0eae",
   "metadata": {},
   "source": [
    "# 평가항목 (2)\n",
    "\n",
    "- 평가문항 : 분류 모델의 F1 score가 기준 이상 높게 나왔는가?\n",
    "- 상세사항 : Vocabulary size에 따른 각 머신러닝 모델의 성능변화 추이를 살피고, 해당 머신러닝 알고리즘의 특성에 근거해 원인을 분석하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ae762",
   "metadata": {},
   "source": [
    "- `F1 단어장 개수별`\n",
    "\n",
    "| 단어장 개수 | MultinomialNB | ComplementNB | DecisionTree | RandomForest | Voting | LogisticRegression | LinearSVC | GradientBoosting | 평균 f1 |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|None|0.07|0.47|0.14|0.44|0.25|0.67|0.64|0.56|0.405 |\n",
    "|15000|0.09|0.47|0.14|0.46|0.29|0.67|0.64|0.59|0.4187 |\n",
    "|10000|0.10|0.48|0.18|0.48|0.33|0.66|0.62|0.59| `0.43` ✅|\n",
    "|5000|0.11|0.48|0.16|0.47|0.35|0.65|0.61|0.57|0.4249 |\n",
    "\n",
    "1. 나이브 베이즈류 모델은 단어장 개수(vocabulary size)가 작아질수록 F1 성능이 향상.\n",
    "2. Voting 모델도 단어장 개수가 작아질수록 F1 성능이 향상되지만, 보팅 모델이 나이브 베이즈류 모델이 많은 것으로 추정됨\n",
    "3. 그 외 모델은 단어장 10,000개에서 F1 성능이 가장 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f63c8",
   "metadata": {},
   "source": [
    "- `F1 클래스별`\n",
    "\n",
    "| Class | MultinomialNB | ComplementNB | DecisionTree | RandomForest | Voting | LogisticRegression | LinearSVC | GradientBoosting | 평균 F1 | 비율(%) |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 0 | 0.00 | 0.76 | 0.00 | 0.64 | 0.67 | 0.76 | 0.78 | 0.75 | 0.545 | 0.61 % |\n",
    "| 1 | 0.64 | 0.74 | 0.53 | 0.62 | 0.67 | 0.76 | 0.75 | 0.72 | 0.6787 | `4.81 %` |\n",
    "| 2 | 0.00 | 0.62 | 0.48 | 0.59 | 0.62 | 0.70 | 0.68 | 0.70 | 0.5487 | 0.82 % |\n",
    "| 3 | 0.85 | 0.90 | 0.87 | 0.86 | 0.88 | 0.92 | 0.92 | 0.89 | 0.8862 | `35.17 %` |\n",
    "| 4 | 0.67 | 0.83 | 0.55 | 0.73 | 0.73 | 0.84 | 0.83 | 0.80 | 0.7474 | `21.7 %` |\n",
    "| 5 | 0.00 | 0.00 | 0.00 | 0.33 | 0.00 | 0.00 | 0.00 | 0.29 | 0.0775 | 0.19 % |\n",
    "| 6 ❓ | 0.00 | 0.93 | 0.78 | 0.69 | 0.67 | 0.93 | 0.83 | 0.87 | 0.7125 | 0.53 % |\n",
    "| 7 | 0.00 | 0.80 | 0.00 | 0.50 | 0.50 | 0.50 | 0.50 | 0.50 | 0.4125 | 0.18 % |\n",
    "| 8 | 0.00 | 0.21 | 0.00 | 0.58 | 0.35 | 0.69 | 0.66 | 0.66 | 0.3937 | `1.55 %` |\n",
    "| 9 | 0.15 | 0.87 | 0.88 | 0.68 | 0.76 | 0.85 | 0.81 | 0.80 | 0.725 | `1.12 %` |\n",
    "| 10 | 0.00 | 0.87 | 0.81 | 0.81 | 0.81 | 0.90 | 0.76 | 0.87 | 0.7287 | `1.38 %` |\n",
    "| 11 | 0.65 | 0.63 | 0.57 | 0.58 | 0.64 | 0.69 | 0.70 | 0.62 | 0.635 | `4.34 %` |\n",
    "| 12 | 0.00 | 0.00 | 0.00 | 0.22 | 0.13 | 0.48 | 0.44 | 0.48 | 0.2187 | 0.55 % |\n",
    "| 13 | 0.05 | 0.59 | 0.00 | 0.45 | 0.36 | 0.62 | 0.58 | 0.54 | 0.3987 | `1.91 %`|\n",
    "| 14 | 0.00 | 0.00 | 0.00 | 0.80 | 0.00 | 0.80 | 0.67 | 0.22 | 0.3112 | 0.29 % |\n",
    "| 15 | 0.00 | 0.18 | 0.00 | 0.20 | 0.00 | 0.57 | 0.63 | 0.38 | 0.245 | 0.22 % |\n",
    "| 16 | 0.61 | 0.72 | 0.68 | 0.58 | 0.71 | 0.74 | 0.70 | 0.73 | 0.6837 | `4.94 %` |\n",
    "| 17 | 0.00 | 0.00 | 0.00 | 0.22 | 0.00 | 0.73 | 0.59 | 0.72 | 0.2825 | 0.43 % |\n",
    "| 18 | 0.00 | 0.57 | 0.00 | 0.42 | 0.09 | 0.74 | 0.72 | 0.60 | 0.3925 | 0.73 % |\n",
    "| 19 | 0.68 | 0.65 | 0.50 | 0.63 | 0.67 | 0.69 | 0.64 | 0.67 | 0.6412 | `6.11 % `|\n",
    "| 20 | 0.08 | 0.43 | 0.10 | 0.45 | 0.33 | 0.55 | 0.51 | 0.52 | 0.3712 | `2.99 %` |\n",
    "| 21 | 0.00 | 0.71 | 0.00 | 0.55 | 0.26 | 0.75 | 0.62 | 0.66 | 0.4437 | `1.11 % `|\n",
    "| 22 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.25 | 0.25 | 0.00 | 0.0625 | 0.17 % |\n",
    "| 23 | 0.00 | 0.60 | 0.00 | 0.35 | 0.13 | 0.69 | 0.56 | 0.58 | 0.3637 | 0.46 % |\n",
    "| 24 | 0.00 | 0.18 | 0.24 | 0.43 | 0.32 | 0.57 | 0.61 | 0.61 | 0.3699 | 0.69 % |\n",
    "| 25 | 0.06 | 0.79 | 0.29 | 0.57 | 0.34 | 0.76 | 0.68 | 0.76 | 0.5312 | `1.02 % `|\n",
    "| 26 | 0.00 | 0.88 | 0.00 | 0.77 | 0.67 | 0.93 | 0.88 | 0.75 | 0.61 | 0.27 % |\n",
    "| 27 | 0.00 | 0.40 | 0.00 | 0.40 | 0.40 | 0.40 | 0.67 | 0.50 | 0.3462 | 0.17 % |\n",
    "| 28 | 0.00 | 0.14 | 0.17 | 0.46 | 0.18 | 0.44 | 0.47 | 0.32 | 0.2724 | 0.53 % |\n",
    "| 29 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.73 | 0.55 | 0.50 | 0.2225 | 0.21 % |\n",
    "| 30 | 0.00 | 0.00 | 0.00 | 0.40 | 0.00 | 0.70 | 0.44 | 0.43 | 0.2462 | 0.5 % |\n",
    "| 31 | 0.00 | 0.47 | 0.00 | 0.12 | 0.00 | 0.64 | 0.55 | 0.48 | 0.2825 | 0.43 % |\n",
    "| 32 | 0.00 | 0.82 | 0.00 | 0.46 | 0.18 | 0.89 | 0.95 | 0.90 | 0.525 | 0.36 % |\n",
    "| 33 ❓| 0.00 | 0.89 | 0.91 | 0.75 | 0.91 | 0.80 | 0.80 | 0.67 | 0.7162 | 0.12 % |\n",
    "| 34  | 0.00 | 0.83 | 0.00 | 0.55 | 0.00 | 0.67 | 0.62 | 0.36 | 0.3787 | 0.56 % |\n",
    "| 35 | 0.00 | 0.29 | 0.00 | 0.29 | 0.29 | 0.50 | 0.29 | 0.73 | 0.2987 | 0.11 % |\n",
    "| 36 | 0.00 | 0.00 | 0.00 | 0.22 | 0.00 | 0.48 | 0.50 | 0.55 | 0.2187 | 0.55 % |\n",
    "| 37 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.50 | 0.50 | 0.80 | 0.225 | 0.21 % |\n",
    "| 38 | 0.00 | 0.50 | 0.00 | 0.40 | 0.00 | 0.40 | 0.80 | 0.33 | 0.3037 | 0.21 % |\n",
    "| 39 | 0.00 | 0.33 | 0.00 | 0.00 | 0.00 | 0.44 | 0.44 | 0.40 | 0.2012 | 0.27 % |\n",
    "| 40 | 0.00 | 0.00 | 0.00 | 0.46 | 0.00 | 0.40 | 0.25 | 0.13 | 0.1550 | 0.4 % |\n",
    "| 41 | 0.00 | 0.36 | 0.00 | 0.33 | 0.00 | 0.50 | 0.71 | 0.35 | 0.2812 | 0.33 % |\n",
    "| 42 | 0.00 | 0.50 | 0.00 | 0.50 | 0.00 | 0.80 | 0.50 | 0.67 | 0.3712 | 0.14 % |\n",
    "| 43 | 0.00 | 0.29 | 0.00 | 0.67 | 0.00 | 0.92 | 0.80 | 0.62 | 0.4125 | 0.23 % |\n",
    "| 44 | 0.00 | 0.73 | 0.00 | 0.89 | 0.89 | 0.73 | 0.62 | 0.73 | 0.5737 | 0.13 % |\n",
    "| 45 ❓ | 0.00 | 1.00 | 0.00 | 1.00 | 1.00 | 1.00 | 0.67 | 1.00 | 0.7087 | 0.2 % |\n",
    "\n",
    "\n",
    "- 결과\n",
    "    - (머신러닝 알고리즘의 특성에 근거해 원인을 분석하기에는 분석 결과에 대한 신뢰성이 떨어짐으로 실행하지 않고, 결과 위주로 분석했습니다)\n",
    "\n",
    "    1. MultinomialNB \n",
    "        - 라벨 벨런싱 문제에 영향을 많이 받는다. 즉, 비율이 높은 라벨은 잘 맞추지만, 낮은 라벨은 전혀 못맞춘다 \n",
    "    2. 라벨 6, 33, 45 의 경우, 비율에 비해 잘 맞추고 있다.\n",
    "        - 왜 그럴까요? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b4df1",
   "metadata": {},
   "source": [
    "# 평가항목 (3)\n",
    "\n",
    "- 평가문항 : 딥러닝 모델을 활용해 성능이 비교 및 확인되었는가?\n",
    "- 상세사항 : 동일한 데이터셋과 전처리 조건으로 딥러닝 모델의 성능과 비교하여 결과에 따른 원인을 분석하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceca4aa",
   "metadata": {},
   "source": [
    "### 실험 조건 \n",
    "\n",
    "1. 동일한 데이터셋 적용\n",
    "2. 전처리 \n",
    "    1. 벡터화 방식 : TF-IDF 미적용 , 데이터셋에서 제공하는 정수 데이터 그대로 사용\n",
    "    2. 텍스트 길이 500 고정\n",
    "    3. 패딩 길이 500 고정\n",
    "    4. 단어장 개수 10,000 고정\n",
    "\n",
    "\n",
    "- 딥러닝 모델 구조\n",
    "    - 주석에 있는 레이어 2개만 RNN, LSTM, 양방향 RNN 으로 수정하면서 실험\n",
    "    - Acc는 Val_acc를 사용했으며, val_acc 중에 max 값을 기준으로 작성함\n",
    "    \n",
    "```\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_len, 32))\n",
    "model.add(SimpleRNN(64, return_sequences=True))  # 👈 변경 부분\n",
    "model.add(SimpleRNN(64))                         # 👈 변경 부분 \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad55973",
   "metadata": {},
   "source": [
    "### 결과 \n",
    "\n",
    "- Acc \n",
    "\n",
    "|머신러닝 모델 |머신러닝 Acc | 딥러닝 모델| 딥러닝 Acc | \n",
    "|:---:|:---:|:---:|:---:|\n",
    "|MultinomialNB|0.2595|LSTM| 0.5620|\n",
    "|ComplementNB|0.3561|RNN| 0.5726|\n",
    "|DecisionTree|0.4118|양방향 RNN | `0.5737` ✅|\n",
    "|RandomForest|0.4269| -| -|\n",
    "|Voting|0.3695|-| -|\n",
    "|평균|0.2280 | 평균 | 0.5694 |\n",
    "\n",
    "- 머신러닝 기법 중에 성능이 가장 좋았던 RF( 0.4269 )를 딥러닝 모델은 모두 넘었다\n",
    "- 딥러닝 레이어가 깊지 않아서 딥러닝 모델 간 성능은 크게 차이가 없는 것 같다\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df34f27",
   "metadata": {},
   "source": [
    "### 실험 코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "749865c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_RNN(num_words=10000, max_len_text=None, max_len_pad=500, vec_type=None):\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(\n",
    "        num_words=num_words,\n",
    "        test_split=0.2\n",
    "    )\n",
    "\n",
    "    # 데이터 길이 제한\n",
    "    if max_len_text:\n",
    "        x_train = [x[:max_len_text] for x in x_train]\n",
    "        x_test = [x[:max_len_text] for x in x_test]\n",
    "\n",
    "    # Padding - TF-IDF 에서는 필요없음.\n",
    "    if vec_type == 'tfidf':\n",
    "        x_train, x_test = tf_idf(x_train, x_test)\n",
    "        x_train = pad_sequences(x_train, maxlen=max_len_pad)\n",
    "        x_test = pad_sequences(x_test, maxlen=max_len_pad)\n",
    "    else:\n",
    "        # Padding - TF-IDF 에서는 필요없음.\n",
    "        x_train = pad_sequences(x_train, maxlen=max_len_pad)\n",
    "        x_test = pad_sequences(x_test, maxlen=max_len_pad)\n",
    "\n",
    "    num_classes = 46\n",
    "    max_len = len(x_train[0])\n",
    "    \n",
    "    # Models\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(max_len, 32))\n",
    "        model.add(Bidirectional(SimpleRNN(64, return_sequences=True)))  # return_sequences=True로 다음 LSTM 레이어에 시퀀스 전달\n",
    "        model.add(Bidirectional(SimpleRNN(64)))  # 두 번째 LSTM 레이어 추가\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['acc'],\n",
    "        )\n",
    "\n",
    "        # Early stopping 콜백을 생성합니다.\n",
    "        early_stopping = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "        history = model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            epochs=50,\n",
    "            batch_size=126,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[\n",
    "                early_stopping,\n",
    "                mc\n",
    "            ],\n",
    "        )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89bd1d04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "58/58 [==============================] - 91s 2s/step - loss: 2.5062 - acc: 0.3520 - val_loss: 2.2224 - val_acc: 0.4574\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.45743, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "58/58 [==============================] - 89s 2s/step - loss: 2.0454 - acc: 0.4816 - val_loss: 1.9150 - val_acc: 0.4891\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.45743 to 0.48915, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      "58/58 [==============================] - 89s 2s/step - loss: 1.7937 - acc: 0.5374 - val_loss: 1.9052 - val_acc: 0.4953\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.48915 to 0.49527, saving model to best_model.h5\n",
      "Epoch 4/50\n",
      "58/58 [==============================] - 89s 2s/step - loss: 1.6940 - acc: 0.5613 - val_loss: 1.7639 - val_acc: 0.5409\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.49527 to 0.54090, saving model to best_model.h5\n",
      "Epoch 5/50\n",
      "58/58 [==============================] - 89s 2s/step - loss: 1.6223 - acc: 0.5795 - val_loss: 1.7044 - val_acc: 0.5609\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.54090 to 0.56093, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      "58/58 [==============================] - 89s 2s/step - loss: 1.5048 - acc: 0.6173 - val_loss: 1.7015 - val_acc: 0.5565\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.56093\n",
      "Epoch 7/50\n",
      "58/58 [==============================] - 89s 2s/step - loss: 1.4493 - acc: 0.6278 - val_loss: 1.8576 - val_acc: 0.5198\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.56093\n",
      "Epoch 8/50\n",
      "58/58 [==============================] - 89s 2s/step - loss: 1.3756 - acc: 0.6465 - val_loss: 1.7015 - val_acc: 0.5737\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.56093 to 0.57373, saving model to best_model.h5\n",
      "Epoch 9/50\n",
      "58/58 [==============================] - 89s 2s/step - loss: 1.2973 - acc: 0.6650 - val_loss: 1.7226 - val_acc: 0.5665\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.57373\n"
     ]
    }
   ],
   "source": [
    "history = process_RNN(num_words=10000, max_len_text=500, max_len_pad=500, vec_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc68950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5737339854240417\n"
     ]
    }
   ],
   "source": [
    "print(max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac7df274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# history 객체로부터 손실 및 정확도 값 얻기\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# 에폭 수\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# 손실 그래프\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745ce52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
