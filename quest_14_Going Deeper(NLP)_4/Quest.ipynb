{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0ec6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow\n",
    "import matplotlib\n",
    "\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "from tqdm import tqdm    \n",
    "import random\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10bdae7",
   "metadata": {},
   "source": [
    "# 12-1. 프로젝트: 한영 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad1a91",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5f193",
   "metadata": {},
   "source": [
    "### 데이터 다운로드\n",
    "\n",
    "- URL로 다운로드 시도했으나, tar.gz 포맷 인식 못해서 수동으로 다운로드 진행함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72b2ed",
   "metadata": {},
   "source": [
    "### 압축 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d16573f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xvf korean-english-park.train.tar.gz\n",
    "# !tar -xvf korean-english-park.test.tar.gz\n",
    "# !tar -xvf korean-english-park.dev.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c2c2f",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제\n",
    "\n",
    "set 데이터형이 중복을 허용하지 않는다는 것을 활용해 중복된 데이터를 제거하도록 합니다. 데이터의 병렬 쌍이 흐트러지지 않게 주의하세요! 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다.\n",
    "\n",
    "앞서 정의한 preprocessing() 함수는 한글에서는 동작하지 않습니다. 한글에 적용할 수 있는 정규식을 추가하여 함수를 재정의하세요!\n",
    "\n",
    "타겟 언어인 영문엔 <start> 토큰과 <end> 토큰을 추가하고 split() 함수를 이용하여 토큰화합니다. 한글 토큰화는 KoNLPy의 mecab 클래스를 사용합니다.\n",
    "\n",
    "모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다. cleaned_corpus로부터 토큰의 길이가 40 이하인 데이터를 선별하여 eng_corpus와 kor_corpus를 각각 구축하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3bbc56",
   "metadata": {},
   "source": [
    "### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f5f5c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'train': {\n",
    "        'ko':[],\n",
    "        'en':[],\n",
    "    },\n",
    "    'test': {\n",
    "        'ko':[],\n",
    "        'en':[],\n",
    "    },\n",
    "    'dev': {\n",
    "        'ko':[],\n",
    "        'en':[],\n",
    "    },\n",
    "}\n",
    "\n",
    "# 파일 열기\n",
    "with open(\"korean-english-park.train.ko\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['train']['ko'].append(line)\n",
    "        \n",
    "with open(\"korean-english-park.train.en\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['train']['en'].append(line)\n",
    "        \n",
    "with open(\"korean-english-park.test.ko\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['test']['ko'].append(line)\n",
    "        \n",
    "with open(\"korean-english-park.test.en\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['test']['en'].append(line)\n",
    "        \n",
    "with open(\"korean-english-park.dev.ko\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['dev']['ko'].append(line)\n",
    "        \n",
    "with open(\"korean-english-park.dev.en\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['dev']['en'].append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af5d614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(\n",
    "    {\n",
    "        'ko':data['train']['ko'],\n",
    "        'en':data['train']['en']\n",
    "    }\n",
    ")\n",
    "test = pd.DataFrame(\n",
    "    {\n",
    "        'ko':data['test']['ko'],\n",
    "        'en':data['test']['en']\n",
    "    }\n",
    ")\n",
    "dev = pd.DataFrame(\n",
    "    {\n",
    "        'ko':data['dev']['ko'],\n",
    "        'en':data['dev']['en']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dcd4f3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - ko : 94124\n",
      "train - en : 94124\n",
      "test - ko : 2001\n",
      "test - en : 2001\n",
      "dev - ko : 1001\n",
      "dev - en : 1001\n"
     ]
    }
   ],
   "source": [
    "print('train - ko :',len(data['train']['ko']))\n",
    "print('train - en :',len(data['train']['en']))\n",
    "print('test - ko :',len(data['test']['ko']))\n",
    "print('test - en :',len(data['test']['en']))\n",
    "print('dev - ko :',len(data['dev']['ko']))\n",
    "print('dev - en :',len(data['dev']['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2cd0711b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94124 -> 13399\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ko</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[개인, 용, 컴퓨터, 사용, 의, 상당, 부분, 은, 이것, 보다, 뛰어날, 수,...</td>\n",
       "      <td>[&lt;start&gt;, much, of, personal, computing, is, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[그러나, 이것, 은, 또한, 책상, 도, 필요, 로, 하, 지, 않, 는다, .]</td>\n",
       "      <td>[&lt;start&gt;, like, all, optical, mice, ,, but, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[결정, 적, 인, 순간, 에, 그, 들, 의, 능력, 을, 증가, 시켜, 줄, 그...</td>\n",
       "      <td>[&lt;start&gt;, something, that, will, boost, their,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[러시아, 특수, 부대, 는, 극장, 으로, 공격, 해, 들어가, 기, 전, 에, ...</td>\n",
       "      <td>[&lt;start&gt;, russian, special, forces, used, a, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[많, 은, 인질, 들, 이, 화학, 가스, 의, 영향, 으로, 고통, 을, 겪, ...</td>\n",
       "      <td>[&lt;start&gt;, many, captives, were, taken, to, hos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ko  \\\n",
       "0   [개인, 용, 컴퓨터, 사용, 의, 상당, 부분, 은, 이것, 보다, 뛰어날, 수,...   \n",
       "2      [그러나, 이것, 은, 또한, 책상, 도, 필요, 로, 하, 지, 않, 는다, .]   \n",
       "8   [결정, 적, 인, 순간, 에, 그, 들, 의, 능력, 을, 증가, 시켜, 줄, 그...   \n",
       "15  [러시아, 특수, 부대, 는, 극장, 으로, 공격, 해, 들어가, 기, 전, 에, ...   \n",
       "16  [많, 은, 인질, 들, 이, 화학, 가스, 의, 영향, 으로, 고통, 을, 겪, ...   \n",
       "\n",
       "                                                   en  \n",
       "0   [<start>, much, of, personal, computing, is, a...  \n",
       "2   [<start>, like, all, optical, mice, ,, but, it...  \n",
       "8   [<start>, something, that, will, boost, their,...  \n",
       "15  [<start>, russian, special, forces, used, a, s...  \n",
       "16  [<start>, many, captives, were, taken, to, hos...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessing(df):\n",
    "    print(f'{len(df)} ->' ,end=' ')\n",
    "    # 중복 제거\n",
    "    df = df.drop_duplicates(subset=['ko'])\n",
    "    df = df.drop_duplicates(subset=['en'])\n",
    "    \n",
    "    # 정규식\n",
    "    def preprocess_sentence_kor(sentence):\n",
    "        sentence = sentence.strip()                                         # 문장의 양쪽 공백 제거\n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)                   # 특수 문자 및 구두점 주변에 공백 추가\n",
    "        sentence = re.sub(r'[\" \"]+', \" \", sentence)                         # 여러 개의 공백을 하나의 공백으로 대체\n",
    "        sentence = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", sentence)  # 한글 및 영어 이외의 문자는 공백으로 대체\n",
    "        sentence = sentence.strip()                                         # 다시 양쪽 공백 제거\n",
    "        return sentence\n",
    "    def preprocess_sentence_eng(sentence):\n",
    "        sentence = sentence.lower().strip()\n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "        sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "        sentence = sentence.strip()\n",
    "        return sentence\n",
    "    \n",
    "    # 'ko'와 'en' 열에 정규식 전처리 함수 적용\n",
    "    df['ko'] = df['ko'].apply(preprocess_sentence_kor)\n",
    "    df['en'] = df['en'].apply(preprocess_sentence_eng)\n",
    "    \n",
    "    # 길이가 0 인 데이터 제거\n",
    "    df = df[(df['ko'].str.len() > 0) & (df['en'].str.len() > 0)]\n",
    "    \n",
    "    # en 에 <start> , <end> 토큰 추가\n",
    "    df['en'] = df['en'].apply(lambda x : '<start> '+ x + ' <end>')\n",
    "    \n",
    "    # 토큰화 , ko : mecab , en : split\n",
    "    mecab = Mecab()\n",
    "    df['ko'] = df['ko'].apply(lambda x : mecab.morphs(x)) # nouns\n",
    "    df['en'] = df['en'].apply(lambda x : x.split())\n",
    "    \n",
    "    # 토큰의 길이가 20 이하인 데이터를 선별하여 \n",
    "    df = df[(df['ko'].str.len() < 21) & (df['en'].str.len() < 21)]\n",
    "    print(f'{len(df)}')\n",
    "    return df\n",
    "\n",
    "train = preprocessing(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "587c43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train[:30000]\n",
    "# print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "81e8a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_corpus와 kor_corpus를 각각 구축하세요.\n",
    "kor_corpus = list(train['ko'].values)\n",
    "eng_corpus = list(train['en'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7b41fd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean: 세균 의 그러 한 적응 능력 이 과학자 들 을 우려 하 게 하 고 있 다 .\n",
      "\n",
      "English: <start> that adaptability worries scientists . <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Korean:\", ' '.join(kor_corpus[180]))\n",
    "print()\n",
    "print(\"English:\", ' '.join(eng_corpus[180]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f0970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7141bb2",
   "metadata": {},
   "source": [
    "# Step 3. 데이터 토큰화\n",
    "\n",
    "앞서 정의한 tokenize() 함수를 사용해 데이터를 텐서로 변환하고 각각의 tokenizer를 얻으세요! 단어의 수는 실험을 통해 적당한 값을 맞춰주도록 합니다! (최소 10,000 이상!)\n",
    "\n",
    "❗ 주의: 난이도에 비해 데이터가 많지 않아 훈련 데이터와 검증 데이터를 따로 나누지는 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b5742ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus, num_words=None):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # pre\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "17fa1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_tensor, kor_tokenizer = tokenize(kor_corpus, None)\n",
    "eng_tensor, eng_tokenizer = tokenize(eng_corpus, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "db1fc1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1117,  562,  371, ...,    0,    0,    0],\n",
       "       [  48,  245,    5, ...,    0,    0,    0],\n",
       "       [ 240,   36,   33, ...,  325,  103,    1],\n",
       "       ...,\n",
       "       [  67,    5, 1128, ...,    0,    0,    0],\n",
       "       [  48,  377, 1124, ...,    0,    0,    0],\n",
       "       [1032, 1048,   31, ...,    1,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5dd9589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 개수 : None\n"
     ]
    }
   ],
   "source": [
    "print('단어 개수 :', kor_tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f559ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b73b5405",
   "metadata": {},
   "source": [
    "# Step 4. 모델 설계\n",
    "\n",
    "한국어를 영어로 잘 번역해 줄 멋진 Attention 기반 Seq2seq 모델을 설계하세요! 앞서 만든 모델에 Dropout 모듈을 추가하면 성능이 더 좋아집니다! Embedding Size와 Hidden Size는 실험을 통해 적당한 값을 맞춰 주도록 합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f68487",
   "metadata": {},
   "source": [
    "## BahdanauAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "31e9bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9409f4b",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2d9fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d779c2",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0b67008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7ceffb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전의 그래프와 세션 초기화\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c5f717cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 30, 1024)\n",
      "Decoder Output: (64, 14859)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행하세요.\n",
    "\n",
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(kor_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(eng_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18040e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807ac17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f296d2da",
   "metadata": {},
   "source": [
    "# Step 5. 훈련하기\n",
    "\n",
    "훈련엔 위에서 사용한 코드를 그대로 사용하되, eval_step() 부분이 없음에 유의합니다! 매 스텝 아래의 예문에 대한 번역을 생성하여 본인이 생각하기에 가장 멋지게 번역한 Case를 제출하세요! (Attention Map을 시각화해보는 것도 재밌을 거예요!)\n",
    "\n",
    "❕ 참고: 데이터의 난이도가 높은 편이므로 생각만큼 결과가 잘 안나올 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b672e014",
   "metadata": {},
   "source": [
    "## Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "39ff079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, # from_logits 는 확률 분포가 Softmax를 거쳐서 들어오는지, 모델의 출력값 그대로 들어오는지를 결정합니다. 우리는 True 로 줬으니 모델의 출력값을 그대로 전달하면 됩니다!\n",
    "    reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fed52d",
   "metadata": {},
   "source": [
    "## train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f47bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function(데코레이터)는 훈련 외적인 텐서플로우 연산을 GPU에서 동작하게 해 훈련을 가속할 수 있도록 도와줍니다\n",
    "\n",
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbeb049",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7de02d56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 210/210 [01:10<00:00,  2.97it/s, Loss 4.4755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . . . . . . . . . . . . . . . . . . . . \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: the lot . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: the . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the lot . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 210/210 [00:38<00:00,  5.45it/s, Loss 3.8201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: the first . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: the first . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: the first . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the first . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 210/210 [00:39<00:00,  5.37it/s, Loss 3.5150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: the new york <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: the world s . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: the world s . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the world s . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 210/210 [00:38<00:00,  5.42it/s, Loss 3.2659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: the world <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: the world s . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: she was in the first , and the first , and the first , and the first , and the \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the first , the first , the first , the first , the first , the first , the first \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 3.0284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: i had a day , he was a day , he was a day , he was a day , \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the hospital . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 210/210 [00:38<00:00,  5.40it/s, Loss 2.7857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: the world s . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: we re going to be a lot of the next door , we re going to be a lot of \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the suspect in the soldiers were killed in the soldiers were killed in the soldiers were killed in the soldiers \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 210/210 [00:38<00:00,  5.39it/s, Loss 2.5388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . not researching the company <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: diarra was a long way to be a long way to be a long way to be a long way \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: we are the next time , we are the next time , we are the next time , we are \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the u . s . government . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 210/210 [00:38<00:00,  5.39it/s, Loss 2.2940]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: the internet . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: we are the rest of the rest of the rest of the rest of the rest of the rest of \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the police were in the police were in the police were in the police were in the police were in \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 2.0574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: i had an expectation that he had an expectation that he had an expectation that he had an expectation that \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the soldiers were killed in the attacker and the attacker and the attacker and the attacker and the attacker and \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 1.8306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: people . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: i had a happy to go to go to go to go to go to go to go to go \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the last week . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 210/210 [00:39<00:00,  5.35it/s, Loss 1.6248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . women interpret the company <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: turkel , with . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: i had a day . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: about , people were made in the bomb were made in the bomb were made in the bomb were made \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 210/210 [00:39<00:00,  5.35it/s, Loss 1.4354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: people , the skin . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: i was a happy to go to go to go to go to go to go to go to go \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the floor , the floor , the floor , the floor , the floor , the floor , the floor \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 1.2616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: people . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and depend largely happy to have compiled a few days . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: about , police were married in the local officials were married in the local officials were married in the local \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 210/210 [00:39<00:00,  5.37it/s, Loss 1.1098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: downtown to the divide between and merry christmas . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and hatreds themselves as a day . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: one of the victims were married in . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.9739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . not researching the company <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: people . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and hatreds recede or in one question <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 210/210 [00:39<00:00,  5.37it/s, Loss 0.8577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . forgetting you re in follow up salary too soon <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and , she had to do the screen . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: one of the seven astronauts were married in five of the seven astronauts were married in five of the seven \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.7490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . not researching the moment you wait for inauguration <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and , she s an act of it s as well as it s as well as it s as \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the victims were married women were married women were married women were married women were married women were married women \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.6660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . mushrooms <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: if you . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and , she had an increased to go , and hatreds cole , she had an increased to go , \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts , five married and five married and five married and five married and five married and five \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.5836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . not targeting your r eacute to . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and , she had an almost an almost an almost an almost an almost an almost an almost an almost \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.5120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . not on the implications of . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: if gone . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s a baby . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married in . <end> \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# be_total_loss = 99999\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, kor_tensor.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(kor_tensor[idx:idx+BATCH_SIZE],\n",
    "                                eng_tensor[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                eng_tokenizer)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "        \n",
    "    translate(\"오바마는 대통령이다.\", encoder, decoder)\n",
    "    translate(\"시민들은 도시 속에 산다.\", encoder, decoder)\n",
    "    translate(\"커피는 필요 없다.\", encoder, decoder)\n",
    "    translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)\n",
    "    \n",
    "    \n",
    "#     if be_total_loss < (total_loss / (batch + 1)):\n",
    "#         break\n",
    "#     else:\n",
    "#         be_total_loss = (total_loss / (batch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ec86a3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 0.4442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it , and it , i didn t like this , i didn t like this , i didn \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts , and five astronauts , and five astronauts , and five astronauts , and five astronauts , \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 210/210 [00:39<00:00,  5.34it/s, Loss 0.3894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it that it it it it it it it it it it it it it it it it it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married in . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 210/210 [00:38<00:00,  5.40it/s, Loss 0.3420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i m going to need to take these days <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it , it , it , it , it , it , it , it , it , it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 210/210 [00:39<00:00,  5.35it/s, Loss 0.3010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . toy creator <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: in the best . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it , and it s it s it s it s it s it s it s it s \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married in . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 0.2623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it , and it , and it , and it , and it , and it , and it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts , and five of the seven astronauts , and five of the seven astronauts , and five \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.2314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . not researching the company <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: in short , the clinic . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married in . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.2040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . not the company <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 0.2038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s called for it s it s an easy as it s called for it s it s \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts , five of the seven astronauts , five of the seven astronauts , five of the seven \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 210/210 [00:39<00:00,  5.37it/s, Loss 0.1751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s not elaborate on it and it s not elaborate on it and it s not elaborate on \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married , and five of the seven astronauts were married , and five of the seven \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 0.1502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i don t have to worry about the company . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: in short , we find than that ? <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it out it , it out an internet can the only the north of my go out and it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: south of the seven astronauts were married , and five had children were married , and five had children were \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 0.1324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . not on a look <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: and merry christmas . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it without it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married , and five had children . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.1172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . take a look <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 210/210 [00:39<00:00,  5.37it/s, Loss 0.1080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . toy creator <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: solomon s . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 0.1056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i hated my kids christmas gifts , . only searching for you have any questions for you have any \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s up the north of it s up the north of it s up the north of it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married , and five had children and three were married , and five had children and \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.1019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i m going to take these days off . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married in . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 0.0982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i hated my kids to take these days off . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s up , it s up , and it s up , it s up , and it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married , and five had children . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 210/210 [00:39<00:00,  5.35it/s, Loss 0.1022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i hated my kids <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s as he had an hour , it s up , and put up , and put up \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: south of . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 0.1056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: hotenough . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.0997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: ibm toward edwards is the concubines , the whole tech is the concubines , the whole tech is the concubines \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married as of the seven astronauts were married as of the seven astronauts were married as \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.0946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . not a tech free break <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: ibm toward . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s after the north of it s up to and hatreds recede or needles . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# be_total_loss = 99999\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, kor_tensor.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(kor_tensor[idx:idx+BATCH_SIZE],\n",
    "                                eng_tensor[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                eng_tokenizer)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "        \n",
    "    translate(\"오바마는 대통령이다.\", encoder, decoder)\n",
    "    translate(\"시민들은 도시 속에 산다.\", encoder, decoder)\n",
    "    translate(\"커피는 필요 없다.\", encoder, decoder)\n",
    "    translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)\n",
    "    \n",
    "    \n",
    "#     if be_total_loss < (total_loss / (batch + 1)):\n",
    "#         break\n",
    "#     else:\n",
    "#         be_total_loss = (total_loss / (batch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f4052b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 0.0882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i m going to take these days off . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: in short , or vitamin d . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s up , and put it , she had no , she had no , she had no \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 210/210 [00:39<00:00,  5.37it/s, Loss 0.0848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i hated my last boss . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: in the head the money . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.0778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i don t have to stress or vitamin deficiency , i don t have to stress or vitamin deficiency \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: you know , you know about pet games ? <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s no better <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 210/210 [00:39<00:00,  5.37it/s, Loss 0.0735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: if you know , many times . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married , and five had children were from the seven astronauts were married , and five \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 210/210 [00:39<00:00,  5.35it/s, Loss 0.0708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i hated my last boss . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: don t ? <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s in their , she had been up and it s , it s in their , she \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: south of the seven astronauts students . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 210/210 [00:39<00:00,  5.35it/s, Loss 0.0687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i don t have to worry about the cover letter <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: and other . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s in their two days to and hatreds recede or it s just like this , the face \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts , were married , . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 210/210 [00:39<00:00,  5.35it/s, Loss 0.0659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i m going to over aggressive in follow your vacation . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided from their potential for well , you know about your male masters ? <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts , had children . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 210/210 [00:39<00:00,  5.37it/s, Loss 0.0675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i m going on . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: you want to protect . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it without it without it without it without it without it without it without it without it without it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married , seven astronauts were married , seven astronauts were married , seven astronauts were married \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 210/210 [00:39<00:00,  5.35it/s, Loss 0.0672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: ibm hasn t ? <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and after it , and it , and it , and it , and it , and it , and \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: south of the seven astronauts were married as of the seven astronauts were married as of the seven astronauts were \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 210/210 [00:39<00:00,  5.35it/s, Loss 0.0694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i don t know anything about the company . i don t know anything about the company . i \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided with good . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s just under the north and it , and it , and it , and it , and \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 210/210 [00:39<00:00,  5.37it/s, Loss 0.0778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i don t have any questions for you . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: you ve got home . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s the hotel for and it s the hotel for and it s the hotel for and it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: south of the seven astronauts . . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.0761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . no , is in bursts <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: calm . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s the north of it , and put it , it s the north of it , and \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 210/210 [00:39<00:00,  5.37it/s, Loss 0.0761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . no , i don t know anything <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: calm . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s up to and put everything and it s up to and put everything and it s up \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married as of the seven astronauts were married as of the seven astronauts were married as \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 0.0715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . take a look <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: your ideal . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s the hotel for it , she had no mistake <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: that region . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 210/210 [00:39<00:00,  5.37it/s, Loss 0.0647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i hated my last boss . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were married , and five had children were married , and five had children were married , \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.0582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i m going to need to take these days off . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: calm . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: other astronauts were married in . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.0549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i don t know anything about the company . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: you could work . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s after all of it s just under the old name or two bit of it s just \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: south . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.0535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . dressing for you . i don t have any questions for you . i don t have any questions \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: your kids . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s just take humans . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: south of the seven astronauts . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.0523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . no , i get insurance company . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: you know about your kids and rural areas ? <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts , people were married , . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s, Loss 0.0496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . i don t have any questions for you . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: provided . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: and it s it s it s it s it s it s it s it s it s it \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the seven astronauts were killed . <end> \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# be_total_loss = 99999\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, kor_tensor.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(kor_tensor[idx:idx+BATCH_SIZE],\n",
    "                                eng_tensor[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                eng_tokenizer)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "        \n",
    "    translate(\"오바마는 대통령이다.\", encoder, decoder)\n",
    "    translate(\"시민들은 도시 속에 산다.\", encoder, decoder)\n",
    "    translate(\"커피는 필요 없다.\", encoder, decoder)\n",
    "    translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)\n",
    "    \n",
    "    \n",
    "#     if be_total_loss < (total_loss / (batch + 1)):\n",
    "#         break\n",
    "#     else:\n",
    "#         be_total_loss = (total_loss / (batch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c03d4",
   "metadata": {},
   "source": [
    "- 메모리 문제\n",
    "    - 데이터 축소 : 약 6만개에서 3만개로 축소\n",
    "    - 토크나이저 단어개수 : 1만개\n",
    "    - 배치 축소 : 64 에서 8\n",
    "\n",
    "- 학습 속도(1ep)\n",
    "    - 데이터 개수 : 약 50,000개(전체) 일 때, 1시간 13분\n",
    "    - 데이터 개수 : 30,000개 일 때, 30분\n",
    "    - 데이터 개수 : 10,000개 일 때, 18분\n",
    "    - 데이터 개수 : 5,000개 일 때, 2분\n",
    "    - 데이터 개수 : 1,000개 일 때, 20초"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80895f1b",
   "metadata": {},
   "source": [
    "### 모델 저장 & 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f508b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 저장\n",
    "# encoder.save_weights('/aiffel/aiffel/s2s_translation/weights/encoder_weights_50000_ep1')\n",
    "# decoder.save_weights('/aiffel/aiffel/s2s_translation/weights/decoder_weights_50000_ep1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42345baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 구조를 불러오고 가중치를 로드하여 모델을 복원\n",
    "# loaded_encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "# loaded_encoder.load_weights('/aiffel/aiffel/s2s_translation/weights/encoder_weights2')\n",
    "\n",
    "# loaded_decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "# loaded_decoder.load_weights('/aiffel/aiffel/s2s_translation/weights/decoder_weights2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84929b0c",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f071bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_eval(txt):\n",
    "    sentence = txt.strip()                                         # 문장의 양쪽 공백 제거\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)                   # 특수 문자 및 구두점 주변에 공백 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                         # 여러 개의 공백을 하나의 공백으로 대체\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", sentence)  # 한글 및 영어 이외의 문자는 공백으로 대체\n",
    "    sentence = sentence.strip()                                         # 다시 양쪽 공백 제거\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9867e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: the year . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: the year . <end> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: the . <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the year . <end> \n"
     ]
    }
   ],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((eng_tensor.shape[-1], kor_tensor.shape[-1]))\n",
    "    \n",
    "    sentence = preprocessing_eval(sentence)\n",
    "    inputs = kor_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=kor_tensor.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(eng_tensor.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += eng_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if eng_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Output: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    #plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "'''\n",
    "K1) 오바마는 대통령이다.\n",
    "K2) 시민들은 도시 속에 산다.\n",
    "K3) 커피는 필요 없다.\n",
    "K4) 일곱 명의 사망자가 발생했다.\n",
    "'''\n",
    "translate(\"오바마는 대통령이다.\", encoder, decoder)\n",
    "translate(\"시민들은 도시 속에 산다.\", encoder, decoder)\n",
    "translate(\"커피는 필요 없다.\", encoder, decoder)\n",
    "translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b0a538",
   "metadata": {},
   "source": [
    "# 보고서\n",
    "\n",
    "## 실험 조건\n",
    "\n",
    "1. 데이터 개수 : 1000 / 5000 / 30000 / 50000(전체)\n",
    "2. 토큰 길이 : 40 / 20 \n",
    "3. 단어장 개수 : 10000 / All\n",
    "4. total_loss(또는 에폭 loss)가 1번 이라도 감소하지 않으면 멈추게 설정\n",
    "\n",
    "## 결론\n",
    "\n",
    "> 아래 실험 결과 토대로 작성함\n",
    "\n",
    "1. epoch이 증가함에 따라 loss는 감소하지만, 성능은 떨어진다 ( 토큰 길이 40개 이하, 데이터 개수 1000개 에폭 10 과 23 비교 )\n",
    "2. 데이터 개수가 증가함에 따라 성능이 향상됨은 아니다 ( 물론, 데이터가 증가함에 따라 epoch을 늘려야하지만, 학습 시간이 오래 걸림에 따라 실험 못해봄)\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8207c099",
   "metadata": {},
   "source": [
    "# 실험 결과\n",
    "\n",
    "- 토큰 길이 40개 이하\n",
    "\n",
    "|데이터 개수|ep| 예문 | 제출 | 파파고 |\n",
    "|:---:|:---:|:---|:---|:---|\n",
    "|1000|10| 오바마는 대통령이다. | the faithful .| 신실한 사람들. |\n",
    "||| 시민들은 도시 속에 산다. | the glass and in the government is a lot of the u .| 유리와 정부에 있는 많은 사람들은 당신입니다. |\n",
    "||| 커피는 필요 없다. | they are about .| 그들은 대략. | \n",
    "||| 일곱 명의 사망자가 발생했다. | the government is a few .| 정부는 소수입니다.| \n",
    "|1000|23| 오바마는 대통령이다. | there|  |\n",
    "||| 시민들은 도시 속에 산다. | urban |  |\n",
    "||| 커피는 필요 없다. | the dawn .|  | \n",
    "||| 일곱 명의 사망자가 발생했다. |the other these business . , there , the other these different argument ....| | \n",
    "|5000|10| 오바마는 대통령이다. | the first of the first of the first...|  |\n",
    "||| 시민들은 도시 속에 산다. | the north korea s .|  |\n",
    "||| 커피는 필요 없다. | but the first of the first but the first of the first but the first of the first|  | \n",
    "||| 일곱 명의 사망자가 발생했다. | the first of the first of the first of the first of the first of the first of| | \n",
    "|50000| 1 |  오바마는 대통령이다. | the , the , the the , the , the the , the , the the , the , the |  |\n",
    "||| 시민들은 도시 속에 산다. | the internet . |  |\n",
    "||| 커피는 필요 없다. | the , the , the the , the , the the , the , the the , the , the |  | \n",
    "||| 일곱 명의 사망자가 발생했다. | the , the , the the , the , the the , the , the the , the , the | | \n",
    "\n",
    "- 토큰 길이 20개 이하(학습 속도가 느려서 줄임) + 단어장 개수 10,000개\n",
    "\n",
    "|데이터 개수|ep| 예문 | 제출 | 파파고 |\n",
    "|:---:|:---:|:---|:---|:---|\n",
    "|13000|10| 오바마는 대통령이다. | .|  |\n",
    "||| 시민들은 도시 속에 산다. |the red sox are also demanding to the divide between the divide between the divide between the divide between the  |  |\n",
    "||| 커피는 필요 없다. |i can be honored |  | \n",
    "||| 일곱 명의 사망자가 발생했다. |the grief stricken crowd spilled outside | | \n",
    "\n",
    "- 토큰 길이 20개 이하(학습 속도가 느려서 줄임) + 단어장 개수 ALL\n",
    "\n",
    "|데이터 개수|ep| 예문 | 제출 | 파파고 |\n",
    "|:---:|:---:|:---|:---|:---|\n",
    "|13000|10| 오바마는 대통령이다. |. |  |\n",
    "||| 시민들은 도시 속에 산다. | i m confident that s native of the game of the game of the game of the game of the |  |\n",
    "||| 커피는 필요 없다. | i think of my career|  | \n",
    "||| 일곱 명의 사망자가 발생했다. | the whale| | \n",
    "\n",
    "- 토큰 길이 20개 이하(학습 속도가 느려서 줄임) + 단어장 개수 ALL + Mecab 명사만 사용 \n",
    "\n",
    "|데이터 개수|ep| 예문 | 제출 | 파파고 |\n",
    "|:---:|:---:|:---|:---|:---|\n",
    "|23000|4| 오바마는 대통령이다. |the year .|  |\n",
    "||| 시민들은 도시 속에 산다. | the . |  |\n",
    "||| 커피는 필요 없다. | the year .|  | \n",
    "||| 일곱 명의 사망자가 발생했다. | the year .| | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a5fc7e",
   "metadata": {},
   "source": [
    "|Num|평가문항\t|상세기준|\n",
    "|:---:|:---|:---|\n",
    "|1| 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 한국어 포함하여 잘 이루어졌다.|\t구두점, 대소문자, 띄어쓰기, 한글 형태소분석 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.|\n",
    "|2| Attentional Seq2seq 모델이 정상적으로 구동된다.|\tseq2seq 모델 훈련 과정에서 training loss가 안정적으로 떨어지면서 학습이 진행됨이 확인되었다.|\n",
    "|3|테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.|\t테스트용 디코더 모델이 정상적으로 만들어져서, 정답과 어느 정도 유사한 영어 번역이 진행됨을 확인하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7c7f2",
   "metadata": {},
   "source": [
    "[공부에 도움되는 사이트](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0131e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae0b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ee5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
