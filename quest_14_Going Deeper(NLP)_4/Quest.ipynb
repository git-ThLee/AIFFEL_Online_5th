{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c3baa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow\n",
    "import matplotlib\n",
    "\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "from tqdm import tqdm    \n",
    "import random\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53281e00",
   "metadata": {},
   "source": [
    "# 12-1. 프로젝트: 한영 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d450f",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef73a508",
   "metadata": {},
   "source": [
    "### 데이터 다운로드\n",
    "\n",
    "- URL로 다운로드 시도했으나, tar.gz 포맷 인식 못해서 수동으로 다운로드 진행함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e9c31b",
   "metadata": {},
   "source": [
    "### 압축 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e846e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xvf korean-english-park.train.tar.gz\n",
    "# !tar -xvf korean-english-park.test.tar.gz\n",
    "# !tar -xvf korean-english-park.dev.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee491bf",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제\n",
    "\n",
    "set 데이터형이 중복을 허용하지 않는다는 것을 활용해 중복된 데이터를 제거하도록 합니다. 데이터의 병렬 쌍이 흐트러지지 않게 주의하세요! 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다.\n",
    "\n",
    "앞서 정의한 preprocessing() 함수는 한글에서는 동작하지 않습니다. 한글에 적용할 수 있는 정규식을 추가하여 함수를 재정의하세요!\n",
    "\n",
    "타겟 언어인 영문엔 <start> 토큰과 <end> 토큰을 추가하고 split() 함수를 이용하여 토큰화합니다. 한글 토큰화는 KoNLPy의 mecab 클래스를 사용합니다.\n",
    "\n",
    "모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다. cleaned_corpus로부터 토큰의 길이가 40 이하인 데이터를 선별하여 eng_corpus와 kor_corpus를 각각 구축하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38544614",
   "metadata": {},
   "source": [
    "### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6de3090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'train': {\n",
    "        'ko':[],\n",
    "        'en':[],\n",
    "    },\n",
    "    'test': {\n",
    "        'ko':[],\n",
    "        'en':[],\n",
    "    },\n",
    "    'dev': {\n",
    "        'ko':[],\n",
    "        'en':[],\n",
    "    },\n",
    "}\n",
    "\n",
    "# 파일 열기\n",
    "with open(\"korean-english-park.train.ko\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['train']['ko'].append(line)\n",
    "        \n",
    "with open(\"korean-english-park.train.en\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['train']['en'].append(line)\n",
    "        \n",
    "with open(\"korean-english-park.test.ko\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['test']['ko'].append(line)\n",
    "        \n",
    "with open(\"korean-english-park.test.en\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['test']['en'].append(line)\n",
    "        \n",
    "with open(\"korean-english-park.dev.ko\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['dev']['ko'].append(line)\n",
    "        \n",
    "with open(\"korean-english-park.dev.en\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read()\n",
    "    for line in lines.split('\\n'):\n",
    "        data['dev']['en'].append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d6e51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(\n",
    "    {\n",
    "        'ko':data['train']['ko'],\n",
    "        'en':data['train']['en']\n",
    "    }\n",
    ")\n",
    "test = pd.DataFrame(\n",
    "    {\n",
    "        'ko':data['test']['ko'],\n",
    "        'en':data['test']['en']\n",
    "    }\n",
    ")\n",
    "dev = pd.DataFrame(\n",
    "    {\n",
    "        'ko':data['dev']['ko'],\n",
    "        'en':data['dev']['en']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af830c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - ko : 94124\n",
      "train - en : 94124\n",
      "test - ko : 2001\n",
      "test - en : 2001\n",
      "dev - ko : 1001\n",
      "dev - en : 1001\n"
     ]
    }
   ],
   "source": [
    "print('train - ko :',len(data['train']['ko']))\n",
    "print('train - en :',len(data['train']['en']))\n",
    "print('test - ko :',len(data['test']['ko']))\n",
    "print('test - en :',len(data['test']['en']))\n",
    "print('dev - ko :',len(data['dev']['ko']))\n",
    "print('dev - en :',len(data['dev']['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "011b41df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94124 -> 13399\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ko</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[개인, 용, 컴퓨터, 사용, 의, 상당, 부분, 은, 이것, 보다, 뛰어날, 수,...</td>\n",
       "      <td>[&lt;start&gt;, Much, of, personal, computing, is, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[그러나, 이것, 은, 또한, 책상, 도, 필요, 로, 하, 지, 않, 는다, .]</td>\n",
       "      <td>[&lt;start&gt;, Like, all, optical, mice, ,, But, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[결정, 적, 인, 순간, 에, 그, 들, 의, 능력, 을, 증가, 시켜, 줄, 그...</td>\n",
       "      <td>[&lt;start&gt;, Something, that, will, boost, their,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[러시아, 특수, 부대, 는, 극장, 으로, 공격, 해, 들어가, 기, 전, 에, ...</td>\n",
       "      <td>[&lt;start&gt;, Russian, special, forces, used, a, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[많, 은, 인질, 들, 이, 화학, 가스, 의, 영향, 으로, 고통, 을, 겪, ...</td>\n",
       "      <td>[&lt;start&gt;, Many, captives, were, taken, to, hos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ko  \\\n",
       "0   [개인, 용, 컴퓨터, 사용, 의, 상당, 부분, 은, 이것, 보다, 뛰어날, 수,...   \n",
       "2      [그러나, 이것, 은, 또한, 책상, 도, 필요, 로, 하, 지, 않, 는다, .]   \n",
       "8   [결정, 적, 인, 순간, 에, 그, 들, 의, 능력, 을, 증가, 시켜, 줄, 그...   \n",
       "15  [러시아, 특수, 부대, 는, 극장, 으로, 공격, 해, 들어가, 기, 전, 에, ...   \n",
       "16  [많, 은, 인질, 들, 이, 화학, 가스, 의, 영향, 으로, 고통, 을, 겪, ...   \n",
       "\n",
       "                                                   en  \n",
       "0   [<start>, Much, of, personal, computing, is, a...  \n",
       "2   [<start>, Like, all, optical, mice, ,, But, it...  \n",
       "8   [<start>, Something, that, will, boost, their,...  \n",
       "15  [<start>, Russian, special, forces, used, a, s...  \n",
       "16  [<start>, Many, captives, were, taken, to, hos...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessing(df):\n",
    "    print(f'{len(df)} ->' ,end=' ')\n",
    "    # 중복 제거\n",
    "    df = df.drop_duplicates(subset=['ko'])\n",
    "    df = df.drop_duplicates(subset=['en'])\n",
    "    \n",
    "    # 정규식\n",
    "    def preprocess_sentence(sentence):\n",
    "        sentence = sentence.strip()                                         # 문장의 양쪽 공백 제거\n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)                   # 특수 문자 및 구두점 주변에 공백 추가\n",
    "        sentence = re.sub(r'[\" \"]+', \" \", sentence)                         # 여러 개의 공백을 하나의 공백으로 대체\n",
    "        sentence = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", sentence)  # 한글 및 영어 이외의 문자는 공백으로 대체\n",
    "        sentence = sentence.strip()                                         # 다시 양쪽 공백 제거\n",
    "        return sentence\n",
    "    \n",
    "    # 'ko'와 'en' 열에 정규식 전처리 함수 적용\n",
    "    df['ko'] = df['ko'].apply(preprocess_sentence)\n",
    "    df['en'] = df['en'].apply(preprocess_sentence)\n",
    "    \n",
    "    # 길이가 0 인 데이터 제거\n",
    "    df = df[(df['ko'].str.len() > 0) & (df['en'].str.len() > 0)]\n",
    "    \n",
    "    # en 에 <start> , <end> 토큰 추가\n",
    "    df['en'] = df['en'].apply(lambda x : '<start> '+ x + ' <end>')\n",
    "    \n",
    "    # 토큰화 , ko : mecab , en : split\n",
    "    mecab = Mecab()\n",
    "    df['ko'] = df['ko'].apply(lambda x : mecab.morphs(x))\n",
    "    df['en'] = df['en'].apply(lambda x : x.split())\n",
    "    \n",
    "    # 토큰의 길이가 20 이하인 데이터를 선별하여 \n",
    "    df = df[(df['ko'].str.len() < 21) & (df['en'].str.len() < 21)]\n",
    "    print(f'{len(df)}')\n",
    "    return df\n",
    "train = preprocessing(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "787dac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train[:30000]\n",
    "# print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed082ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_corpus와 kor_corpus를 각각 구축하세요.\n",
    "kor_corpus = list(train['ko'].values)\n",
    "eng_corpus = list(train['en'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c8929a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean: 세균 의 그러 한 적응 능력 이 과학자 들 을 우려 하 게 하 고 있 다 .\n",
      "\n",
      "English: <start> That adaptability worries scientists . <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Korean:\", ' '.join(kor_corpus[180]))\n",
    "print()\n",
    "print(\"English:\", ' '.join(eng_corpus[180]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120350e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d106ebc",
   "metadata": {},
   "source": [
    "# Step 3. 데이터 토큰화\n",
    "\n",
    "앞서 정의한 tokenize() 함수를 사용해 데이터를 텐서로 변환하고 각각의 tokenizer를 얻으세요! 단어의 수는 실험을 통해 적당한 값을 맞춰주도록 합니다! (최소 10,000 이상!)\n",
    "\n",
    "❗ 주의: 난이도에 비해 데이터가 많지 않아 훈련 데이터와 검증 데이터를 따로 나누지는 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c724e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus, num_words=None):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # pre\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4757ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_tensor, kor_tokenizer = tokenize(kor_corpus, None)\n",
    "eng_tensor, eng_tokenizer = tokenize(eng_corpus, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0dfc404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1117,  562,  371, ...,    0,    0,    0],\n",
       "       [  48,  245,    5, ...,    0,    0,    0],\n",
       "       [ 240,   36,   33, ...,  325,  103,    1],\n",
       "       ...,\n",
       "       [  67,    5, 1128, ...,    0,    0,    0],\n",
       "       [  48,  377, 1124, ...,    0,    0,    0],\n",
       "       [1032, 1048,   31, ...,    1,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e207065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 개수 : None\n"
     ]
    }
   ],
   "source": [
    "print('단어 개수 :', kor_tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead21733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "757e72d8",
   "metadata": {},
   "source": [
    "# Step 4. 모델 설계\n",
    "\n",
    "한국어를 영어로 잘 번역해 줄 멋진 Attention 기반 Seq2seq 모델을 설계하세요! 앞서 만든 모델에 Dropout 모듈을 추가하면 성능이 더 좋아집니다! Embedding Size와 Hidden Size는 실험을 통해 적당한 값을 맞춰 주도록 합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d34d58",
   "metadata": {},
   "source": [
    "## BahdanauAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10c87ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e078ed44",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afe50d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ef1f7",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6faa2fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e80fd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전의 그래프와 세션 초기화\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "507ef345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 30, 1024)\n",
      "Decoder Output: (64, 14859)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행하세요.\n",
    "\n",
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(kor_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(eng_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e5ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48f7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b4142ab",
   "metadata": {},
   "source": [
    "# Step 5. 훈련하기\n",
    "\n",
    "훈련엔 위에서 사용한 코드를 그대로 사용하되, eval_step() 부분이 없음에 유의합니다! 매 스텝 아래의 예문에 대한 번역을 생성하여 본인이 생각하기에 가장 멋지게 번역한 Case를 제출하세요! (Attention Map을 시각화해보는 것도 재밌을 거예요!)\n",
    "\n",
    "❕ 참고: 데이터의 난이도가 높은 편이므로 생각만큼 결과가 잘 안나올 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ffbf0",
   "metadata": {},
   "source": [
    "## Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04f2b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, # from_logits 는 확률 분포가 Softmax를 거쳐서 들어오는지, 모델의 출력값 그대로 들어오는지를 결정합니다. 우리는 True 로 줬으니 모델의 출력값을 그대로 전달하면 됩니다!\n",
    "    reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0ac58",
   "metadata": {},
   "source": [
    "## train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3988abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function(데코레이터)는 훈련 외적인 텐서플로우 연산을 GPU에서 동작하게 해 훈련을 가속할 수 있도록 도와줍니다\n",
    "\n",
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8fe727",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384744fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 210/210 [01:10<00:00,  2.96it/s, Loss 4.5492]\n",
      "Epoch  2: 100%|██████████| 210/210 [00:38<00:00,  5.41it/s, Loss 4.0566]\n",
      "Epoch  3: 100%|██████████| 210/210 [00:38<00:00,  5.40it/s, Loss 3.7385]\n",
      "Epoch  4: 100%|██████████| 210/210 [00:38<00:00,  5.41it/s, Loss 3.5189]\n",
      "Epoch  5: 100%|██████████| 210/210 [00:38<00:00,  5.40it/s, Loss 3.3259]\n",
      "Epoch  6: 100%|██████████| 210/210 [00:38<00:00,  5.41it/s, Loss 3.1576]\n",
      "Epoch  7: 100%|██████████| 210/210 [00:38<00:00,  5.42it/s, Loss 2.9955]\n",
      "Epoch  8: 100%|██████████| 210/210 [00:38<00:00,  5.39it/s, Loss 2.8402]\n",
      "Epoch  9: 100%|██████████| 210/210 [00:38<00:00,  5.39it/s, Loss 2.6810]\n",
      "Epoch 10: 100%|██████████| 210/210 [00:38<00:00,  5.39it/s, Loss 2.5287]\n",
      "Epoch 11: 100%|██████████| 210/210 [00:38<00:00,  5.40it/s, Loss 2.3730]\n",
      "Epoch 12: 100%|██████████| 210/210 [00:38<00:00,  5.40it/s, Loss 2.2202]\n",
      "Epoch 13: 100%|██████████| 210/210 [00:38<00:00,  5.39it/s, Loss 2.0763]\n",
      "Epoch 14: 100%|██████████| 210/210 [00:38<00:00,  5.39it/s, Loss 1.9434]\n",
      "Epoch 15: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 1.8212]\n",
      "Epoch 16: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 1.7083]\n",
      "Epoch 17: 100%|██████████| 210/210 [00:38<00:00,  5.39it/s, Loss 1.6082]\n",
      "Epoch 18: 100%|██████████| 210/210 [00:38<00:00,  5.39it/s, Loss 1.5184]\n",
      "Epoch 19: 100%|██████████| 210/210 [00:39<00:00,  5.38it/s, Loss 1.4297]\n",
      "Epoch 20: 100%|██████████| 210/210 [00:38<00:00,  5.39it/s, Loss 1.3502]\n",
      "Epoch 21:  92%|█████████▏| 193/210 [00:35<00:03,  5.40it/s, Loss 1.2757]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "be_total_loss = 99999\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, kor_tensor.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(kor_tensor[idx:idx+BATCH_SIZE],\n",
    "                                eng_tensor[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                eng_tokenizer)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    \n",
    "    \n",
    "    if be_total_loss < (total_loss / (batch + 1)):\n",
    "        break\n",
    "    else:\n",
    "        be_total_loss = (total_loss / (batch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae970b1",
   "metadata": {},
   "source": [
    "- 메모리 문제\n",
    "    - 데이터 축소 : 약 6만개에서 3만개로 축소\n",
    "    - 토크나이저 단어개수 : 1만개\n",
    "    - 배치 축소 : 64 에서 8\n",
    "\n",
    "- 학습 속도(1ep)\n",
    "    - 데이터 개수 : 약 50,000개(전체) 일 때, 1시간 13분\n",
    "    - 데이터 개수 : 30,000개 일 때, 30분\n",
    "    - 데이터 개수 : 10,000개 일 때, 18분\n",
    "    - 데이터 개수 : 5,000개 일 때, 2분\n",
    "    - 데이터 개수 : 1,000개 일 때, 20초"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa87546",
   "metadata": {},
   "source": [
    "### 모델 저장 & 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a21cac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 저장\n",
    "# encoder.save_weights('/aiffel/aiffel/s2s_translation/weights/encoder_weights_50000_ep1')\n",
    "# decoder.save_weights('/aiffel/aiffel/s2s_translation/weights/decoder_weights_50000_ep1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b5733dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7feb801f0dc0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 모델 구조를 불러오고 가중치를 로드하여 모델을 복원\n",
    "# loaded_encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "# loaded_encoder.load_weights('/aiffel/aiffel/s2s_translation/weights/encoder_weights2')\n",
    "\n",
    "# loaded_decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "# loaded_decoder.load_weights('/aiffel/aiffel/s2s_translation/weights/decoder_weights2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6251aee",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6159f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_eval(txt):\n",
    "    sentence = txt.strip()                                         # 문장의 양쪽 공백 제거\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)                   # 특수 문자 및 구두점 주변에 공백 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                         # 여러 개의 공백을 하나의 공백으로 대체\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", sentence)  # 한글 및 영어 이외의 문자는 공백으로 대체\n",
    "    sentence = sentence.strip()                                         # 다시 양쪽 공백 제거\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84b1dddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: . <end> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: i m confident that s native of the game of the game of the game of the game of the \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: i think of my career <end> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: the whale . <end> \n"
     ]
    }
   ],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((eng_tensor.shape[-1], kor_tensor.shape[-1]))\n",
    "    \n",
    "    sentence = preprocessing_eval(sentence)\n",
    "    inputs = kor_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=kor_tensor.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(eng_tensor.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += eng_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if eng_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Output: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    #plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "'''\n",
    "K1) 오바마는 대통령이다.\n",
    "K2) 시민들은 도시 속에 산다.\n",
    "K3) 커피는 필요 없다.\n",
    "K4) 일곱 명의 사망자가 발생했다.\n",
    "'''\n",
    "translate(\"오바마는 대통령이다.\", encoder, decoder)\n",
    "translate(\"시민들은 도시 속에 산다.\", encoder, decoder)\n",
    "translate(\"커피는 필요 없다.\", encoder, decoder)\n",
    "translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac238e",
   "metadata": {},
   "source": [
    "# 보고서\n",
    "\n",
    "## 실험 조건\n",
    "\n",
    "1. 데이터 개수 : 1000 / 5000 / 30000 / 50000(전체)\n",
    "2. 토큰 길이 : 40 / 20 \n",
    "3. 단어장 개수 : 10000 / All\n",
    "4. total_loss(또는 에폭 loss)가 1번 이라도 감소하지 않으면 멈추게 설정\n",
    "\n",
    "## 결론\n",
    "\n",
    "> 아래 실험 결과 토대로 작성함\n",
    "\n",
    "1. epoch이 증가함에 따라 loss는 감소하지만, 성능은 떨어진다 ( 토큰 길이 40개 이하, 데이터 개수 1000개 에폭 10 과 23 비교 )\n",
    "2. 데이터 개수가 증가함에 따라 성능이 향상됨은 아니다 ( 물론, 데이터가 증가함에 따라 epoch을 늘려야하지만, 학습 시간이 오래 걸림에 따라 실험 못해봄)\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f0f2d",
   "metadata": {},
   "source": [
    "# 실험 결과\n",
    "\n",
    "- 토큰 길이 40개 이하\n",
    "\n",
    "|데이터 개수|ep| 예문 | 제출 | 파파고 |\n",
    "|:---:|:---:|:---|:---|:---|\n",
    "|1000|10| 오바마는 대통령이다. | the faithful .| 신실한 사람들. |\n",
    "||| 시민들은 도시 속에 산다. | the glass and in the government is a lot of the u .| 유리와 정부에 있는 많은 사람들은 당신입니다. |\n",
    "||| 커피는 필요 없다. | they are about .| 그들은 대략. | \n",
    "||| 일곱 명의 사망자가 발생했다. | the government is a few .| 정부는 소수입니다.| \n",
    "|1000|23| 오바마는 대통령이다. | there|  |\n",
    "||| 시민들은 도시 속에 산다. | urban |  |\n",
    "||| 커피는 필요 없다. | the dawn .|  | \n",
    "||| 일곱 명의 사망자가 발생했다. |the other these business . , there , the other these different argument ....| | \n",
    "|5000|10| 오바마는 대통령이다. | the first of the first of the first...|  |\n",
    "||| 시민들은 도시 속에 산다. | the north korea s .|  |\n",
    "||| 커피는 필요 없다. | but the first of the first but the first of the first but the first of the first|  | \n",
    "||| 일곱 명의 사망자가 발생했다. | the first of the first of the first of the first of the first of the first of| | \n",
    "|50000| 1 |  오바마는 대통령이다. | the , the , the the , the , the the , the , the the , the , the |  |\n",
    "||| 시민들은 도시 속에 산다. | the internet . |  |\n",
    "||| 커피는 필요 없다. | the , the , the the , the , the the , the , the the , the , the |  | \n",
    "||| 일곱 명의 사망자가 발생했다. | the , the , the the , the , the the , the , the the , the , the | | \n",
    "\n",
    "- 토큰 길이 20개 이하(학습 속도가 느려서 줄임) + 단어장 개수 10,000개\n",
    "\n",
    "|데이터 개수|ep| 예문 | 제출 | 파파고 |\n",
    "|:---:|:---:|:---|:---|:---|\n",
    "|13000|10| 오바마는 대통령이다. | .|  |\n",
    "||| 시민들은 도시 속에 산다. |the red sox are also demanding to the divide between the divide between the divide between the divide between the  |  |\n",
    "||| 커피는 필요 없다. |i can be honored |  | \n",
    "||| 일곱 명의 사망자가 발생했다. |the grief stricken crowd spilled outside | | \n",
    "\n",
    "- 토큰 길이 20개 이하(학습 속도가 느려서 줄임) + 단어장 개수 ALL\n",
    "\n",
    "|데이터 개수|ep| 예문 | 제출 | 파파고 |\n",
    "|:---:|:---:|:---|:---|:---|\n",
    "|13000|10| 오바마는 대통령이다. |. |  |\n",
    "||| 시민들은 도시 속에 산다. | i m confident that s native of the game of the game of the game of the game of the |  |\n",
    "||| 커피는 필요 없다. | i think of my career|  | \n",
    "||| 일곱 명의 사망자가 발생했다. | the whale| | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca5f37",
   "metadata": {},
   "source": [
    "|Num|평가문항\t|상세기준|\n",
    "|:---:|:---|:---|\n",
    "|1| 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 한국어 포함하여 잘 이루어졌다.|\t구두점, 대소문자, 띄어쓰기, 한글 형태소분석 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.|\n",
    "|2| Attentional Seq2seq 모델이 정상적으로 구동된다.|\tseq2seq 모델 훈련 과정에서 training loss가 안정적으로 떨어지면서 학습이 진행됨이 확인되었다.|\n",
    "|3|테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.|\t테스트용 디코더 모델이 정상적으로 만들어져서, 정답과 어느 정도 유사한 영어 번역이 진행됨을 확인하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc1d25",
   "metadata": {},
   "source": [
    "[공부에 도움되는 사이트](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46c435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4152c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d6d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
